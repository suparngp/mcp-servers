`/`
[Product docs](/docs/home)[Guides](/docs/guides)[SDKs](/docs/sdk)[Integrations](/docs/integrations)[API docs](/docs/api)[Tutorials](/docs/tutorials)[Flagship Blog](/docs/blog)
 * Getting started
 * [Overview](/docs/home)
 * [Launch Insights](/docs/home/getting-started/launch-insights)
 * [LaunchDarkly architecture](/docs/home/getting-started/architecture)
 * [LaunchDarkly vocabulary](/docs/home/getting-started/vocabulary)
 * Feature flags and AI Configs
 * [Create flags](/docs/home/flags/create)
 * [Target with flags](/docs/home/flags/target)
 * [Flag templates](/docs/home/flags/templates)
 * [Manage flags](/docs/home/flags/manage)
 * [Code references](/docs/home/flags/code-references)
 * [AI Configs](/docs/home/ai-configs)
 * [Contexts](/docs/home/flags/contexts)
 * [Segments](/docs/home/flags/segments)
 * Releases
 * [Releasing features with LaunchDarkly](/docs/home/releases/releasing)
 * [Release policies](/docs/home/releases/release-policies)
 * [Percentage rollouts](/docs/home/releases/percentage-rollouts)
 * [Progressive rollouts](/docs/home/releases/progressive-rollouts)
 * [Guarded rollouts](/docs/home/releases/guarded-rollouts)
 * [Feature monitoring](/docs/home/releases/feature-monitoring)
 * [Release pipelines](/docs/home/releases/release-pipelines)
 * [Engineering insights](/docs/home/releases/eng-insights)
 * [Release management tools](/docs/home/releases/release-management)
 * [Applications and app versions](/docs/home/releases/apps-and-app-versions)
 * [Change history](/docs/home/releases/change-history)
 * Observability
 * [Observability](/docs/home/observability)
 * [Session replay](/docs/home/observability/session-replay)
 * [Error monitoring](/docs/home/observability/errors)
 * [Logs](/docs/home/observability/logs)
 * [Traces](/docs/home/observability/traces)
 * [LLM observability](/docs/home/observability/llm-observability)
 * [Alerts](/docs/home/observability/alerts)
 * [Dashboards](/docs/home/observability/dashboards)
 * [Search specification](/docs/home/observability/search)
 * [Observability settings](/docs/home/observability/settings)
 * [Vega](/docs/home/observability/vega)
 * Product analytics
 * [Product analytics](/docs/home/product-analytics)
 * [Setting up product analytics](/docs/home/product-analytics/setup)
 * [Using product analytics charts](/docs/home/product-analytics/chart)
 * Experimentation
 * [Experimentation](/docs/home/experimentation)
 * [Creating experiments](/docs/home/experimentation/create)
 * [Managing experiments](/docs/home/experimentation/manage)
 * [Analyzing experiments](/docs/home/experimentation/analyze)
 * [Experimentation and metric events](/docs/home/experimentation/events)
 * [Multi-armed bandits](/docs/home/multi-armed-bandits)
 * [Holdouts](/docs/home/holdouts)
 * Metrics
 * [Metrics](/docs/home/metrics)
 * [Metric groups](/docs/home/metrics/metric-groups)
 * [Autogenerated metrics](/docs/home/metrics/autogen-metrics)
 * [Metric impact](/docs/home/metrics/metric-impact)
 * [Metric events](/docs/home/metrics/metric-events)
 * Warehouse native
 * [Warehouse native Experimentation](/docs/home/warehouse-native)
 * [Warehouse native metrics](/docs/home/warehouse-native/metrics)
 * [Creating warehouse native experiments](/docs/home/warehouse-native/creating)
 * Infrastructure
 * [Connect apps and services to LaunchDarkly](/docs/home/infrastructure/apps)
 * [LaunchDarkly in China and Pakistan](/docs/home/infrastructure/china)
 * [LaunchDarkly in the European Union (EU)](/docs/home/infrastructure/eu)
 * [LaunchDarkly in federal environments](/docs/home/infrastructure/federal)
 * [Public IP list](/docs/home/infrastructure/ip-list)
 * Your account
 * [Projects](/docs/home/account/project)
 * [Views](/docs/home/account/views)
 * [Environments](/docs/home/account/environment)
 * [Tags](/docs/home/account/tags)
 * [Teams](/docs/home/account/teams)
 * [Members](/docs/home/account/members)
 * [Roles](/docs/home/account/roles)
 * [Account security](/docs/home/account/secure)
 * [Billing and usage](/docs/home/account/billing)
 * [Changelog](/docs/home/changelog)
[Sign in](/)[Sign up](https://app.launchdarkly.com/signup)
On this page
 * [Overview](#overview)
 * [A](#a)
 * [AI Config](#ai-config)
 * [Alert](#alert)
 * [Analysis method](#analysis-method)
 * [Application](#application)
 * [Arm averages graph](#arm-averages-graph)
 * [Attribute](#attribute)
 * [Audience](#audience)
 * [B](#b)
 * [Base role](#base-role)
 * [Bayesian statistics](#bayesian-statistics)
 * [Boolean flag](#boolean-flag)
 * [C](#c)
 * [Cohort](#cohort)
 * [Confidence interval](#confidence-interval)
 * [Context](#context)
 * [Context instance](#context-instance)
 * [Context kind](#context-kind)
 * [Control variation](#control-variation)
 * [Conversion rate](#conversion-rate)
 * [Conversions](#conversions)
 * [Credible interval](#credible-interval)
 * [Custom role](#custom-role)
 * [D](#d)
 * [Dashboard](#dashboard)
 * [Default rule](#default-rule)
 * [E](#e)
 * [Environment](#environment)
 * [Error](#error)
 * [Evaluation](#evaluation)
 * [Event](#event)
 * [Event key](#event-key)
 * [Expected loss](#expected-loss)
 * [Experiment](#experiment)
 * [Experiment flag](#experiment-flag)
 * [Experimentation key](#experimentation-key)
 * [Exposures](#exposures)
 * [F](#f)
 * [Fallback value](#fallback-value)
 * [Fallthrough rule](#fallthrough-rule)
 * [Feature change experiment](#feature-change-experiment)
 * [Flag](#flag)
 * [Flag exposure](#flag-exposure)
 * [Forest plot](#forest-plot)
 * [Frequentist statistics](#frequentist-statistics)
 * [Funnel metric group](#funnel-metric-group)
 * [Funnel optimization experiment](#funnel-optimization-experiment)
 * [G](#g)
 * [Guarded release or guarded rollout](#guarded-release-or-guarded-rollout)
 * [H](#h)
 * [Holdout](#holdout)
 * [Hypothesis](#hypothesis)
 * [I](#i)
 * [Iteration](#iteration)
 * [J](#j)
 * [JSON flag](#json-flag)
 * [K](#k)
 * [Kill switch](#kill-switch)
 * [L](#l)
 * [Layer](#layer)
 * [Log](#log)
 * [M](#m)
 * [Mean](#mean)
 * [Member](#member)
 * [Metric](#metric)
 * [Metric estimate](#metric-estimate)
 * [Migration flag](#migration-flag)
 * [Observability settings](#observability-settings)
 * [Monthly active users (MAU)](#monthly-active-users-mau)
 * [Mutually exclusive experiment](#mutually-exclusive-experiment)
 * [N](#n)
 * [Number flag](#number-flag)
 * [O](#o)
 * [Organization role](#organization-role)
 * [P](#p)
 * [P-value](#p-value)
 * [Percentage rollout](#percentage-rollout)
 * [Policy (custom role)](#policy-custom-role)
 * [Posterior distribution](#posterior-distribution)
 * [Posterior mean](#posterior-mean)
 * [Preset roles](#preset-roles)
 * [Prerequisite](#prerequisite)
 * [Primary context kind](#primary-context-kind)
 * [Probability density graph](#probability-density-graph)
 * [Probability to be best](#probability-to-be-best)
 * [Probability to be worse](#probability-to-be-worse)
 * [Probability to beat control](#probability-to-beat-control)
 * [Progressive release or progressive rollout](#progressive-release-or-progressive-rollout)
 * [Project](#project)
 * [Project role](#project-role)
 * [R](#r)
 * [Randomization unit](#randomization-unit)
 * [Regression](#regression)
 * [Regression threshold](#regression-threshold)
 * [Relative difference](#relative-difference)
 * [Relative difference graph](#relative-difference-graph)
 * [Relative difference from control](#relative-difference-from-control)
 * [Release flag](#release-flag)
 * [Release pipeline](#release-pipeline)
 * [Release policy](#release-policy)
 * [Role](#role)
 * [Role attribute](#role-attribute)
 * [Role scope](#role-scope)
 * [Rule](#rule)
 * [S](#s)
 * [Sample ratio mismatch (SRM)](#sample-ratio-mismatch-srm)
 * [Saved search](#saved-search)
 * [SDK](#sdk)
 * [Search specification](#search-specification)
 * [Segment](#segment)
 * [Session](#session)
 * [Session replay](#session-replay)
 * [Significance level](#significance-level)
 * [Span](#span)
 * [Standard metric group](#standard-metric-group)
 * [Statistical significance](#statistical-significance)
 * [String flag](#string-flag)
 * [T](#t)
 * [Target](#target)
 * [Team](#team)
 * [Threshold](#threshold)
 * [Total value](#total-value)
 * [Trace](#trace)
 * [Traffic allocation](#traffic-allocation)
 * [Treatment variation](#treatment-variation)
 * [U](#u)
 * [Unit aggregation method](#unit-aggregation-method)
 * [User](#user)
 * [V](#v)
 * [Variation](#variation)
 * [View](#view)
 * [W](#w)
 * [Winning variation](#winning-variation)
## Overview
This topic defines common words used in the LaunchDarkly application and documentation. While many of these words may be familiar to you, in some cases they have nuances specific to LaunchDarkly.
The following definitions may be useful as you work in LaunchDarkly:
## A
### AI Config
An `AI Config` is a LaunchDarkly resource that you create when your application uses artificial intelligence (AI) model generation. It manages the model configurations and messages you use in your application. When you use a LaunchDarkly [AI SDK](/docs/sdk/ai) to [customize an AI Config](/docs/sdk/features/ai-config), the SDK determines which message and model your application should serve to which [contexts](/docs/home/getting-started/vocabulary#context). The SDK also customizes the message based on context attributes that you provide.
To learn more, read [AI Configs](/docs/home/ai-configs) and [AI SDKs](/docs/sdk/ai).
### Alert
An `alert` is a notification LaunchDarkly sends when observability data meets a specific condition. Alerts are based on sessions, errors, logs, traces, or events within session replays.
An alert includes these components:
 * [Thresholds](/docs/home/getting-started/vocabulary#threshold): The numeric values that trigger the alert condition.
 * **Windows** : The time ranges LaunchDarkly searches and aggregates data for before deciding if the alert condition is met.
 * **Cooldown periods** : The amount of time after a notification is sent before LaunchDarkly can send another notification for the same alert.
 * **Recipients** : The members of your organization who receive notifications when the alert condition is met. Notifications are delivered by email and, if configured, through Slack.
Alerts are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Alerts](/docs/home/observability/alerts).
### Analysis method
A [metric’s](/docs/home/getting-started/vocabulary#metric) `analysis method` is the mathematical technique by which you want to analyze its results. You can analyze results by mean, median, or percentile.
To learn more, read [Analysis method](/docs/home/metrics/metric-analysis#analysis-method).
### Application
An `application` is a LaunchDarkly resource that describes what you are delivering to a customer. LaunchDarkly automatically creates applications when it establishes a connection with a LaunchDarkly SDK that contains application information. After an application is created, you can build flag targeting rules based on application name, version, or other properties, such as whether or not a particular application version is supported.
To learn more, read [Applications and application versions](/docs/home/releases/apps-and-app-versions).
### Arm averages graph
In LaunchDarkly [experiment](/docs/home/getting-started/vocabulary#experiment) results, the arm averages graph displays the average value over time for each variation. This graph is useful for investigating trends that impact all experiment variations equally over time.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Attribute
An `attribute` is a field in a [context](/docs/home/getting-started/vocabulary#context) that you can use in targeting rules for flags and segments. Each context that encounters a feature flag in your product can have a different value for a given attribute.
To learn more, read [Context attributes](/docs/home/flags/context-attributes).
### Audience
An [experiment’s](/docs/home/getting-started/vocabulary#experiment) `audience` is the combination of the targeting rule you’re experimenting on and the number of [contexts](/docs/home/getting-started/vocabulary#context) you [allocate](/docs/home/getting-started/vocabulary#traffic-allocation) to each flag [variation](/docs/home/getting-started/vocabulary#variation).
To learn more, read [Allocating experiment audiences](/docs/home/experimentation/allocation).
## B
### Base role
A `base role` is a set of permissions that describes the access a member has. LaunchDarkly provides Reader, Writer, Admin, and Owner base roles. Some customers also have a restricted No access role.
To learn more, read [Base roles](/docs/home/account/roles/role-concepts#base-roles). For details on the permissions in each base role, read [Organization roles](/docs/home/account/roles/organization-roles).
### Bayesian statistics
In LaunchDarkly [Experimentation](/docs/home/getting-started/vocabulary#experiment), `Bayesian statistics` is a results analysis option good for experiments with small sample sizes. The other analysis option is [frequentist statistics](/docs/home/getting-started/vocabulary#frequentist-statistics).
To learn more, read [Bayesian versus frequentist statistics](/docs/guides/experimentation/bayesian-frequentist).
### Boolean flag
A `boolean flag` is a feature flag type with two variations that hold `true` and `false`.
Boolean flags can be used for on/off switches, new feature rollouts, or anything that requires a binary option.
To learn more, read [Flag types](/docs/home/flags/types).
## C
### Cohort
A `cohort` can refer to a group of contexts in Amplitude synced with a LaunchDarkly [segment](/docs/home/getting-started/vocabulary#segment), or to the targeting rules on a [migration flag](/docs/home/getting-started/vocabulary#migration-flag).
To learn more about Amplitude cohorts, read [Syncing segments with Amplitude cohorts](/docs/home/flags/amplitude).
Migration flag cohorts are analogous to the targeting rules for other types of feature flags. The default cohort is analogous to the default rule.
To learn more, read [Targeting with migration flags](/docs/home/flags/target).
### Confidence interval
In [frequentist statistics](/docs/home/getting-started/vocabulary#frequentist-statistics), the `confidence interval` is the range of values within which the true [metric](/docs/home/getting-started/vocabulary#metric) value is likely to fall if you were to repeat the experiment many times. For example, a 95% confidence interval means that, in repeated experiments, 95% of the calculated intervals would contain the true value of the metric. To learn more, read [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Context
A `context` is a generalized way of referring to the people, services, machines, or other resources that encounter feature flags in your product.
To learn more, read [Contexts](/docs/home/flags/contexts).
### Context instance
A `context instance` is a unique combination of one or more contexts that have encountered a feature flag in your product.
To learn more, read [Context instances](/docs/home/flags/context-instances).
### Context kind
A `context kind` organizes your [contexts](/docs/home/getting-started/vocabulary#context) into different types based on the kinds of resources encountering flags. Each context has one `kind` with a unique set of corresponding attributes that you can use for targeting and Experimentation.
Some customers are billed by contexts. This billing method uses [Monthly active users (MAU)](/docs/home/getting-started/vocabulary#monthly-active-users-mau).
To learn more, read [Context kinds](/docs/home/flags/context-kinds).
### Control variation
The `control variation` is typically the existing flag [variation](/docs/home/getting-started/vocabulary#variation) that serves as the benchmark for a [progressive release](/docs/home/getting-started/vocabulary#progressive-release-or-progressive-rollout) or [experiment](/docs/home/getting-started/vocabulary#experiment). Metric values from the control variation determine whether the [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation) has a negative or positive effect on the [metric](/docs/home/getting-started/vocabulary#metric) being measured. It is the departure point of the progressive release.
In [multivariate flags](/docs/home/getting-started/vocabulary#flag), the person configuring the release can designate any variation as the control when setting up the release.
To learn more, read [Experimentation](/docs/home/experimentation).
### Conversion rate
In [Bayesian](/docs/home/getting-started/vocabulary#bayesian-statistics) results, the `conversion rate` column displays the following:
 * For count conversion metrics: the total number of times a context triggered a conversion [event](/docs/home/getting-started/vocabulary#event)
 * For binary conversion metrics: the percentage of contexts that triggered at least one conversion event
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Conversions
In [experiment](/docs/home/getting-started/vocabulary#experiment) results, the `conversions` column displays the total number of times a context triggered a conversion [event](/docs/home/getting-started/vocabulary#event) measured by a conversion [metric](/docs/home/getting-started/vocabulary#metric).
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Credible interval
In [Bayesian statistics](/docs/home/getting-started/vocabulary#bayesian-statistics), the `credible interval` is the range of values within which the true [metric](/docs/home/getting-started/vocabulary#metric) value is likely to fall, given the observed data and prior beliefs. For example, a 90% credible interval means that there is a 90% probability that the true value lies within this range, based on the posterior distribution. To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Custom role
A `custom role` is a set of permissions that you define that describes the access a member has. When you create a custom role, you define the access using a set of statements. Each statement comprises resources and actions. Together, these are called a [policy](/docs/home/getting-started/vocabulary#policy-custom-role).
To learn more, read [Roles](/docs/home/account/roles).
## D
### Dashboard
An observability `dashboard` is a configurable view of observability data in LaunchDarkly. Dashboards include graphs that visualize data about sessions, errors, logs, or traces. Dashboards track error rates, latency, or user engagement over time. Observability dashboards are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Dashboards](/docs/home/observability/dashboards).
### Default rule
A `default rule` describes the feature flag [variation](/docs/home/getting-started/vocabulary#variation) to serve to the contexts that don’t match any of the individual targets or rules you have previously specified for the flag. It is sometimes called the “fallthrough” rule because all of the rules preceding it have been evaluated, and the context encountering the flag has “fallen through” to this last rule. To learn more, read [Set the default rule](/docs/home/flags/default-rule).
The default rule only applies when the flag is toggled on. If the flag is toggled off, then LaunchDarkly will serve the “default off variation” for the flag. In the LaunchDarkly user interface (UI), the default off variation is specified in the field labeled “If targeting is off, serve.” To learn more, read [The off variation](/docs/home/flags/off-variation).
## E
### Environment
An `environment` is an organizational unit contained within a [project](/docs/home/getting-started/vocabulary#project). You can create multiple environments within each project. Environments in LaunchDarkly typically correspond to the environments in which your code is deployed, such as development, staging, and production. All environments in a single project contain the same [flags](/docs/home/getting-started/vocabulary#flag). However, the flags can have different states, targets, and rules in each environment.
To learn more, read [Environments](/docs/home/account/environment).
### Error
An `error` is an issue in your application that LaunchDarkly records through observability SDKs. LaunchDarkly groups errors based on their messages and stack traces, so you can see how often they occur and what contexts they affect. Error details include the number of affected contexts, when the error was first and most recently observed, and whether it has been resolved. Errors are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Error monitoring](/docs/home/observability/errors).
### Evaluation
An `evaluation` is what happens when:
 * your application’s code sends the [LaunchDarkly SDK](/docs/home/getting-started/vocabulary#sdk) information about a particular [flag](/docs/home/getting-started/vocabulary#flag) or [AI Config](/docs/home/getting-started/vocabulary#ai-config) and a particular [context](/docs/home/getting-started/vocabulary#context) that has encountered it, and
 * the SDK sends back the value of the [variation](/docs/home/getting-started/vocabulary#variation) that the context should receive.
We say that the SDK _evaluates_ the flag, or that the flag has been evaluated for a particular context or customer. We say that the SDK _customizes_ the AI Config, because for AI Configs the SDK both sends back the value of the variation and also customizes that value based on context attributes.
![](https://fern-image-hosting.s3.us-east-1.amazonaws.com/launchdarkly/terminal.svg)
Try it in your SDK: [Evaluating flags](/docs/sdk/features/evaluating), [Customizing AI Configs](/docs/sdk/features/ai-config)
### Event
An `event` refers to data that LaunchDarkly [SDKs](/docs/home/getting-started/vocabulary#sdk) send to LaunchDarkly when a user or other [context](/docs/home/getting-started/vocabulary#context) takes an action in your app. Server-side, client-side, and edge SDKs send analytics events to LaunchDarkly as a result of feature flag evaluations and certain SDK calls.
To learn more, read [Analytics events](/docs/sdk/concepts/events) and [Metric events](/docs/home/metrics/metric-events).
### Event key
An `event key` is a unique identifier you set for a particular kind of event within your app. [Metrics](/docs/home/getting-started/vocabulary#metric) use event keys to identify events for performance tracking. Events are [environment-specific](/docs/home/getting-started/vocabulary#environment).
To learn more, read [Metric events](/docs/home/metrics/metric-events).
### Expected loss
The `expected loss` for a [variation](/docs/home/getting-started/vocabulary#variation) within an [experiment](/docs/home/getting-started/vocabulary#experiment) is the risk, expressed as a percentage, that the variation will not actually be an improvement over the [control variation](/docs/home/getting-started/vocabulary#control-variation) due to the margin of error in metric results. Expected loss displays only for metrics that use an “Average” [analysis method](/docs/home/getting-started/vocabulary#analysis-method).
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Experiment
An `experiment` is a LaunchDarkly feature that connects a [flag](/docs/home/getting-started/vocabulary#flag) or [AI Config](/docs/home/getting-started/vocabulary#ai-config) with one or more [metrics](/docs/home/getting-started/vocabulary#metric) to measure end-user behavior. Experiments track how different [variations](/docs/home/getting-started/vocabulary#variation) affect end-user interactions with your app, and determine the [winning variation](/docs/home/getting-started/vocabulary#winning-variation). You can run an experiment for one or more [iterations](/docs/home/getting-started/vocabulary#iteration).
To learn more, read [Experimentation](/docs/home/experimentation).
### Experiment flag
An `experiment flag` is a temporary flag that has an experiment running on one of its targeting rules.
To learn more, read [Flag templates](/docs/home/flags/templates).
### Experimentation key
An `experimentation key` is a unique [context](/docs/home/getting-started/vocabulary#context) key from a server-side, client-side, AI, or edge SDK, that is included in each experiment:
 * If the same context key is in one experiment multiple times, LaunchDarkly counts it as one Experimentation key.
 * If the same context key is in two different experiments, LaunchDarkly counts it as two Experimentation keys.
Some customers are billed by Experimentation keys. To learn more, read [Experimentation keys](/docs/home/account/calculating-billing#experimentation-keys).
### Exposures
In LaunchDarkly [experiments](/docs/home/getting-started/vocabulary#experiment), `exposures` are the total number of contexts included in the experiment over time.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
## F
### Fallback value
The `fallback value` is the value your application should use for a feature flag or AI Config in error situations.
Specifically, for the client-side, server-side, and edge SDKs, the `fallback value` is the flag [variation](/docs/home/getting-started/vocabulary#variation) that LaunchDarkly serves in the following two situations:
 * If your application cannot connect to LaunchDarkly.
 * If your application can connect to LaunchDarkly, but the flag is toggled off and you have not specified a default off variation. To learn more, read [The off variation](/docs/home/flags/off-variation).
For the AI SDKs, the `fallback value` is the value of the variation of your [AI Config](/docs/home/getting-started/vocabulary#ai-config) to use if the AI Config is not found or if any errors occur during processing.
Regardless of how you configure variations or targeting rules, each time you evaluate a flag or customize an AI Config from the LaunchDarkly SDK, you must include a fallback value as one of the parameters.
![](https://fern-image-hosting.s3.us-east-1.amazonaws.com/launchdarkly/terminal.svg)
Try it in your SDK: [Evaluating flags](/docs/sdk/features/evaluating), [Customizing AI Configs](/docs/sdk/features/ai-config)
### Fallthrough rule
A `fallthrough rule` is a synonym for [default rule](/docs/home/getting-started/vocabulary#default-rule).
### Feature change experiment
A `feature change experiment` lets you measure the effect different flag [variations](/docs/home/getting-started/vocabulary#variation) have on a [metric](/docs/home/getting-started/vocabulary#metric).
To learn more, read [Experiment types](/docs/home/experimentation/types).
### Flag
A `flag` is the basic unit of feature management. It describes the different variations of a feature and the rules that allow different entities to access them. Different entities that access your features could be a percentage of your application’s traffic, individuals, or people or software entities who share common characteristics like location, email domain, or type of mobile device. The entities that encounter feature flags in your product are called [contexts](/docs/home/getting-started/vocabulary#context).
To learn more, read [Using feature management](/docs/home/getting-started/feature-management).
### Flag exposure
A `flag exposure` occurs when a unique [context](/docs/home/getting-started/vocabulary#context) encounters a [flag](/docs/home/getting-started/vocabulary#flag) for the first time and becomes part of the experiment population. Exposures determine which contexts receive the [control variation](/docs/home/getting-started/vocabulary#control-variation) or the [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation).
In a [guarded rollout](/docs/home/getting-started/vocabulary#guarded-release-or-guarded-rollout):
 * If the first stage is set to 1%, then 2% of the flag’s population participates in the experiment, with 1% receiving the control variation and 1% receiving the treatment variation.
 * Guarded rollouts maintain an equal ratio of contexts between variations to ensure fair comparison. When this ratio becomes imbalanced, it creates a [sample ratio mismatch (SRM)](/docs/home/getting-started/vocabulary#sample-ratio-mismatch-srm).
 * For conversion metrics, all flag exposures count toward that metric’s exposure total.
 * For other metric types, exposures count only when the contexts generate a [metric event](/docs/home/getting-started/vocabulary#event).
To learn more, read [Guarded rollouts](/docs/home/releases/guarded-rollouts).
### Forest plot
In LaunchDarkly frequentist [experiment](/docs/home/getting-started/vocabulary#experiment) results, the forest plot shows the confidence interval for each treatment variation with respect to the control variation. The forest plot should be your main decision-making tool when deciding on a winning variation for frequentist experiments.
To learn more, read [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Frequentist statistics
In LaunchDarkly [Experimentation](/docs/home/getting-started/vocabulary#experiment), `frequentist statistics` is a results analysis option good for experiments with larger sample sizes. The other analysis option is [Bayesian statistics](/docs/home/getting-started/vocabulary#bayesian-statistics).
### Funnel metric group
A `funnel metric group` is a reusable, ordered list of metrics you can use with [funnel optimization experiments](/docs/home/getting-started/vocabulary#funnel-optimization-experiment) to measure end user progression through a number of steps, typically from the awareness stage to the purchasing stage of your marketing funnel.
To learn more, read [Metric groups](/docs/home/metrics/metric-groups).
### Funnel optimization experiment
A `funnel optimization experiment` uses multiple [metrics](/docs/home/getting-started/vocabulary#metric) within a [funnel metric group](/docs/home/getting-started/vocabulary#funnel-metric-group) to track the performance of each of the steps in a marketing funnel over time.
To learn more, read [Experiment types](/docs/home/experimentation/types).
## G
### Guarded release or guarded rollout
A `guarded release` or `guarded rollout` is a [progressive release](/docs/home/getting-started/vocabulary#progressive-release-or-progressive-rollout) that includes attached [metrics](/docs/home/getting-started/vocabulary#metric). These metrics run a structured A/B experiment to detect if the [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation) has caused a [regression](/docs/home/getting-started/vocabulary#regression). In a guarded rollout, LaunchDarkly gradually increases the percentage of [contexts](/docs/home/getting-started/vocabulary#context) that are receiving a particular flag [variation](/docs/home/getting-started/vocabulary#variation) while monitoring those metrics for regressions. To perform this monitoring, you must attach one or more metrics to your flag. You can configure LaunchDarkly to notify you or automatically roll back a release when it detects a regression.
To learn more, read [Guarded rollouts](/docs/home/releases/guarded-rollouts) and [Releasing features with LaunchDarkly](/docs/home/releases/releasing).
## H
### Holdout
A `holdout` is a group of [contexts](/docs/home/getting-started/vocabulary#context) that you have temporarily excluded from all or a selected set of your [experiments](/docs/home/getting-started/vocabulary#experiment). Holdouts allow you to measure the effectiveness of your Experimentation program.
To learn more, read [Holdouts](/docs/home/holdouts).
### Hypothesis
In LaunchDarkly, a `hypothesis` is a theory or assumption that can be tested with an [experiment](/docs/home/getting-started/vocabulary#experiment). A hypothesis should be specific and answer a single question. You can phrase a hypothesis for a LaunchDarkly experiment as:
> If [I make a specific change to our codebase], then [one or more measurable metrics will improve] because [the change had this effect].
To learn more, read [Formulate a hypothesis](/docs/guides/experimentation/designing-experiments#formulate-a-hypothesis).
## I
### Iteration
An `iteration` is a defined time period that you run an [experiment](/docs/home/getting-started/vocabulary#experiment) for. An iteration can be any length that you choose, and you can run multiple iterations of the same experiment.
To learn more, read [Managing experiments](/docs/home/experimentation/manage).
## J
### JSON flag
A `JSON flag` is a feature flag type with variations that hold structured data as a JSON object or array. They are multivariate flags, meaning they can have more than two variations. JSON flags have a size limit of 32KB.
JSON flags are useful when you need to bundle multiple related values within a single variation, such as settings for a UI layout or an API response.
To learn more, read [Flag types](/docs/home/flags/types).
## K
### Kill switch
A `kill switch` is a permanent flag used to shut off tools or functionality in the case of an emergency.
To learn more, read [Flag templates](/docs/home/flags/templates).
## L
### Layer
A `layer` is a set of [experiments](/docs/home/getting-started/vocabulary#experiment) that cannot share traffic with each other. All of the experiments within a layer are [mutually exclusive](/docs/home/getting-started/vocabulary#mutually-exclusive-experiment), which means that if a context is included in one experiment, LaunchDarkly will exclude it from any other experiments in the same layer.
To learn more, read [Mutually exclusive experiments](/docs/home/experimentation/mutually-exclusive).
### Log
A `log` is an event in your application that LaunchDarkly records through observability SDKs. Logs include attributes such as level, message, service name, environment, file path, line number, and session ID. Logs help you troubleshoot issues and trace events back to related sessions. Logs are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Logs](/docs/home/observability/logs).
## M
### Mean
In [frequentist](/docs/home/getting-started/vocabulary#frequentist-statistics) experiment results, the `mean` is a flag [variation’s](/docs/home/getting-started/vocabulary#variation) average numeric value that you should expect in an [experiment](/docs/home/getting-started/vocabulary#experiment), based on the data collected so far. Only numeric [metrics](/docs/home/getting-started/vocabulary#metric) measure the mean value.
To learn more, read [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Member
A `member` or `account member` is a person who uses LaunchDarkly at your organization. These people work at your organization or have access rights to your organization’s LaunchDarkly environment for another reason, such as contractors or part-time employees.
To learn more, read [Members](/docs/home/account/members).
### Metric
A `metric` is a curated event query that defines a specific time series of data based on an underlying [event](/docs/home/getting-started/vocabulary#event). A metric defines what you want to measure and the [context kind](/docs/home/getting-started/vocabulary#context-kind) used as the analysis unit. Metrics have three types: count, binary, and numeric.
LaunchDarkly uses different kinds of metrics to measure the impact of variation changes, gauge application performance, track account usage, and more.
The different kinds of metrics within LaunchDarkly include:
 * **Experimentation and guarded rollout metrics** : these metrics allow you to measure specific end-user behaviors as part of an [experiment](/docs/home/getting-started/vocabulary#experiment) or [guarded rollout](/docs/home/getting-started/vocabulary#guarded-release-or-guarded-rollout). Metrics can measure things like links clicked, money spent, or response time. When combined with a [flag](/docs/home/getting-started/vocabulary#flag) or an [AI Config](/docs/home/getting-started/vocabulary#ai-config) in an experiment, metrics determine which [variation](/docs/home/getting-started/vocabulary#variation) is the [winning variation](/docs/home/getting-started/vocabulary#winning-variation). Metrics send metric [events](/docs/home/getting-started/vocabulary#event) to LaunchDarkly. To learn more, read [Metrics](/docs/home/metrics).
 * **AI Config metrics** : these metrics are automatically created by LaunchDarkly AI SDKs and monitor the performance of an AI Config. AI Config metrics are available in the LaunchDarkly UI if you [track AI metrics in your SDK](/docs/sdk/features/ai-metrics).
 * **Migration flag metrics** : these metrics track the progress of a migration flag. To learn more, read [Migration flag metrics](/docs/home/flags/migration-metrics).
 * **Application adoption metrics** : these metrics track the adoption percentage for an application version. To learn more, read [Adoption metrics](/docs/home/releases/apps-and-app-versions#adoption-metrics).
 * **Account metrics** : these metrics help you understand your monthly active users (MAU) usage, Experimentation key usage, Data Export usage, and server usage for billing purposes. To learn more, read [Account usage metrics](/docs/home/account/metrics).
 * **Launch Insights metrics** : these metrics summarize how your organization is adopting the best practices associated with risk-free releases. To learn more, read [Launch Insights](/docs/home/getting-started/launch-insights).
### Metric estimate
A `metric estimate` is the current expected value for a [metric](/docs/home/getting-started/vocabulary#metric). For mean metrics, this value is the [posterior mean](/docs/home/getting-started/vocabulary#posterior-mean), which represents the average value of the posterior data for the metric.
To learn more, read [Analyzing experiments](/docs/home/experimentation/analyze).
### Migration flag
A `migration flag` is a temporary flag used to migrate data or systems while keeping your application available and disruption free. Migration flags break up the switch from an old to a new implementation into a series of recommended stages where movement from one stage to the next is done in incremental steps.
To learn more, read [Flag templates](/docs/home/flags/templates).
### Observability settings
`Observability settings` are project-level settings that control how LaunchDarkly ingests and manages observability data.
Observability settings include these components:
 * **Filters** : Rules that exclude sessions, errors, logs, or traces before ingestion. Filtered data does not count against observability quotas.
 * **Sampling** : A percentage value that controls how many records LaunchDarkly ingests. Sampling helps reduce data volume.
 * **Rage click detection** : Configurable options for detecting repeated end-user clicks that indicate frustration. Settings include elapsed time, click radius, and minimum number of clicks.
 * **Source maps** : Uploaded files that let LaunchDarkly display error stack traces in human-readable form.
 * **Auto-resolution rules** : Options to automatically resolve errors that have not occurred for a defined period of time.
Observability settings are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Observability settings](/docs/home/observability/settings).
### Monthly active users (MAU)
`MAU` is a billing metric that measures the number of user contexts your flags encounter from client-side and edge SDKs over a particular month. MAU includes user contexts that are both single contexts and those that are part of a multi-context. These user contexts appear on the **Contexts** list, and expire from the list after 30 days of inactivity.
To learn more, read [Account usage metrics](/docs/home/account/metrics).
### Mutually exclusive experiment
A `mutually exclusive experiment` is an [experiment](/docs/home/getting-started/vocabulary#experiment) configured to prevent its [contexts](/docs/home/getting-started/vocabulary#context) from being included in other experiments. Experiments are mutually exclusive from each other when they are contained within the same [layer](/docs/home/getting-started/vocabulary#layer).
To learn more, read [Mutually exclusive experiments](/docs/home/experimentation/mutually-exclusive).
## N
### Number flag
A `number flag` is a feature flag type with variations that hold numerical integer or floating point values. They are multivariate flags, meaning they can have more than two variations.
Number flags are useful for numerical configuration parameters, such as thresholds or timeouts.
To learn more, read [Flag types](/docs/home/flags/types).
## O
### Organization role
An `organization role` is a type of [role](/docs/home/getting-started/vocabulary#role). It is a set of permissions that you can assign to an account member. LaunchDarkly provides several of these as [preset roles](/docs/home/getting-started/vocabulary#preset-roles), including Billing Admin, Admin, Architect, and Member.
To learn more, read [Roles](/docs/home/account/roles), [Organization roles](/docs/home/account/roles/organization-roles), and [Member role concepts](/docs/home/account/roles/role-concepts).
## P
### P-value
In [frequentist](/docs/home/getting-started/vocabulary#frequentist-statistics) experiment results, a [treatment variation’s](/docs/home/getting-started/vocabulary#treatment-variation) probability value, or `p-value`, is the likelihood that the observed difference from the [control variation](/docs/home/getting-started/vocabulary#control-variation) is due to random chance. A p-value of less than or equal to 0.05 is statistically significant. The lower the p-value, the less likely the [relative difference](/docs/home/getting-started/vocabulary#relative-difference-from-control) is due to chance alone.
### Percentage rollout
A `percentage rollout` is a rollout option for a targeting [rule](/docs/home/getting-started/vocabulary#rule) that serves a given flag [variation](/docs/home/getting-started/vocabulary#variation) to a specified percentage of contexts that encounter the flag. A common use case for percentage rollouts is to manually increment the percentage of customers targeted by a flag over time until 100% of the customers receive one variation of a flag.
To learn more, read [Percentage rollouts](/docs/home/releases/percentage-rollouts) and [Releasing features with LaunchDarkly](/docs/home/releases/releasing).
### Policy (custom role)
A `policy` is part of a [custom role](/docs/home/getting-started/vocabulary#custom-role). A policy combines resources and actions into a set of statements that define what members can or cannot do in LaunchDarkly. You can create policies using the [policy builder](/docs/home/account/roles/role-create#create-policies-for-roles) or the [advanced editor](/docs/home/account/roles/advanced-editor).
To learn more, read [Roles](/docs/home/account/roles) and [Using policies](/docs/home/account/roles/role-policies).
### Posterior distribution
In [Bayesian statistics](/docs/home/getting-started/vocabulary#bayesian-statistics), the `posterior distribution` is the expected range of outcomes for a [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation) based on prior beliefs about your data gathered from the control variation.
To learn more, read [Bayesian versus frequentist statistics](/docs/guides/experimentation/bayesian-frequentist).
### Posterior mean
In [Bayesian](/docs/home/getting-started/vocabulary#bayesian-statistics) experiment results, the `posterior mean` is a flag [variation’s](/docs/home/getting-started/vocabulary#variation) average numeric value that you should expect in an [experiment](/docs/home/getting-started/vocabulary#experiment), based on the data collected so far. Only numeric [metrics](/docs/home/getting-started/vocabulary#metric) measure the posterior mean.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Preset roles
A `preset role` refers to a [project role](/docs/home/getting-started/vocabulary#project-role) or [organization role](/docs/home/getting-started/vocabulary#organization-role) that LaunchDarkly provides for customers on select plans. You can assign a preset role to members or teams in your account. You can also add [policy statements](/docs/home/getting-started/vocabulary#policy-custom-role) to a preset role. However, you cannot remove existing policy statements from a preset role.
To learn more, read [Preset roles](/docs/home/account/roles/role-concepts#preset-roles). For details on the permissions in each preset role, read [Project roles](/docs/home/account/roles/project-roles) and [Organization roles](/docs/home/account/roles/organization-roles).
### Prerequisite
You can make flags depend on other flags being enabled to take effect. A `prerequisite` flag is one on which a second flag depends. When the second flag is evaluated, the prerequisite flag must be on, and the [target](/docs/home/getting-started/vocabulary#target) must be receiving the variation of the prerequisite flag that you specify. If the prerequisite flag is toggled off, the target will receive the default off variation of the dependent flag.
To learn more, read [Flag prerequisites](/docs/home/flags/prereqs).
### Primary context kind
The `primary context kind` is the [context kind](/docs/home/getting-started/vocabulary#context-kind) with the highest volume of monthly activity. For most customers, the primary context kind is `user`.
For billing purposes, LaunchDarkly only charges for contexts from the primary context kind, called [MAU](/docs/home/getting-started/vocabulary#monthly-active-users-mau). LaunchDarkly calculates this as the context kind with the largest number of unique contexts that evaluate, initialize, or identify any flag from a client-side SDK over a given calendar month.
To learn more about context kinds, read [Context kinds](/docs/home/flags/context-kinds). To learn more about billing by contexts, read [Calculating billing](/docs/home/account/calculating-billing).
### Probability density graph
In LaunchDarkly Bayesian [experiment](/docs/home/getting-started/vocabulary#experiment) results, the probability density graph displays fine differences in probability for each experiment variation over time.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Probability to be best
In [Bayesian](/docs/home/getting-started/vocabulary#bayesian-statistics) experiment results, the [variation](/docs/home/getting-started/vocabulary#variation) with the highest `probability to be best` is the variation that has the largest positive impact on the metric you’re measuring. Probability to be best displays only for metrics that use an “Average” [analysis method](/docs/home/getting-started/vocabulary#analysis-method).
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Probability to be worse
In LaunchDarkly’s [Bayesian model](/docs/guides/experimentation/bayesian-frequentist), `probability to be worse` indicates how likely it is that the [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation) performs worse than the [control variation](/docs/home/getting-started/vocabulary#control-variation). LaunchDarkly calculates this value by drawing numerous samples from the posterior distributions of both variations and calculating how often the treatment’s sampled value is worse than the control’s by more than the defined [regression threshold](/docs/home/getting-started/vocabulary#threshold).
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Probability to beat control
In [experiment](/docs/home/getting-started/vocabulary#experiment) results, a [treatment variation’s](/docs/home/getting-started/vocabulary#treatment-variation) `probability to beat control` is the likelihood that the variation is better than the control variation.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Progressive release or progressive rollout
A `progressive release` or `progressive rollout` is the delivery of a feature flag rule that gradually increases traffic from a low percentage up to 100%. In a progressive rollout, LaunchDarkly gradually increases the percentage of [contexts](/docs/home/getting-started/vocabulary#context) that are receiving a particular flag [variation](/docs/home/getting-started/vocabulary#variation). You can specify the duration and how the percentage increases. A common use case for progressive rollouts is to automatically increment the percentage of customers targeted by a flag over time until 100% of the customers receive one variation of a flag.
To learn more, read [Progressive rollouts](/docs/home/releases/progressive-rollouts) and [Releasing features with LaunchDarkly](/docs/home/releases/releasing).
### Project
A `project` is an organizational unit in your LaunchDarkly account. Many LaunchDarkly resources, such as [flags](/docs/home/getting-started/vocabulary#flag) and [AI Configs](/docs/home/getting-started/vocabulary#ai-config), are unique at the project level. Customers on plans that allow creating your own [roles](/docs/home/getting-started/vocabulary#custom-role) can choose to limit access for members or teams based on project.
You can define projects in any way you like. A common pattern is to create one project in your LaunchDarkly account for each product your company makes. Each project has multiple [environments](/docs/home/getting-started/vocabulary#environment), and may include multiple [views](/docs/home/getting-started/vocabulary#view).
To learn more, read [Projects](/docs/home/account/project).
### Project role
A `project role` is a type of [role](/docs/home/getting-started/vocabulary#role). It is a set of permissions that you can assign to an account member. LaunchDarkly provides several of these as [preset roles](/docs/home/getting-started/vocabulary#preset-roles), including Project Admin, Contributor, Developer, Maintainer, and Viewer. Customers on select plans can also create their own project roles.
To learn more, read [Roles](/docs/home/account/roles), [Project roles](/docs/home/account/roles/project-roles), and [Member role concepts](/docs/home/account/roles/role-concepts).
## R
### Randomization unit
An [experiment’s](/docs/home/getting-started/vocabulary#experiment) `randomization unit` is the [context kind](/docs/home/getting-started/vocabulary#context-kind) that the experiment uses to randomly sort [contexts](/docs/home/getting-started/vocabulary#context) into each [variation](/docs/home/getting-started/vocabulary#variation), according to the experiment’s [traffic allocation](/docs/home/getting-started/vocabulary#traffic-allocation).
A [guarded rollout’s](/docs/home/getting-started/vocabulary#guarded-release-or-guarded-rollout) `randomization unit` is the [context kind](/docs/home/getting-started/vocabulary#context-kind) that the rollout’s [flag](/docs/home/getting-started/vocabulary#flag) and [metrics](/docs/home/getting-started/vocabulary#metric) use when tracking a flag variation’s performance over time.
Any percentage-based release requires a defined randomization unit. This unit is the chosen context kind used both to randomize the exposure traffic for the release and to measure the associated [metrics](/docs/home/getting-started/vocabulary#metric).
To learn more, read [Randomization unit](/docs/home/metrics/metric-analysis#randomization-unit).
### Regression
A `regression` is when LaunchDarkly detects a negative effect on your application performance as a result of a flag change or rollout. You can use guarded rollouts to notify you or automatically roll back a release when it detects a regression.
To learn more, read [Guarded rollouts](/docs/home/releases/guarded-rollouts).
### Regression threshold
The `regression threshold` defines how much worse a [metric](/docs/home/getting-started/vocabulary#metric) can perform before LaunchDarkly considers it a [regression](/docs/home/getting-started/vocabulary#regression). It is configured per metric as a relative percentage difference. For example, a regression threshold can define whether a 1% decrease is acceptable or whether a 5% decrease indicates a regression.
To learn more, read [Regression thresholds](/docs/home/releases/regression-thresholds).
### Relative difference
The `relative difference` is the percentage difference between the [metric estimates](/docs/home/getting-started/vocabulary#metric-estimate) for the [control variation](/docs/home/getting-started/vocabulary#control-variation) and the [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation). Relative difference measures how much the treatment’s performance differs from the control’s benchmark.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Relative difference graph
In LaunchDarkly [experiment](/docs/home/getting-started/vocabulary#experiment) results, the relative difference graph displays a time series of the relative difference between the treatment variation and the control. This graph is helpful for investigating trends in relative differences over time.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Relative difference from control
In [experiment](/docs/home/getting-started/vocabulary#experiment) results, the `relative difference from control` column displays how much a metric in the treatment variation differs from the control variation, expressed as a proportion of the control’s estimated value.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Release flag
A `release flag` is a temporary flag that initially serves “Unavailable” (false) to most or all of its [targets](/docs/home/getting-started/vocabulary#target), then gradually rolls out the “Available” (true) variation until it reaches 100%.
To learn more, read [Flag templates](/docs/home/flags/templates).
### Release pipeline
A `release pipeline` lets you move flags through a series of phases, rolling out flags to selected [environments](/docs/home/getting-started/vocabulary#environment) and audiences following automated steps. You can use release pipelines to view the status of ongoing releases across all flags within a [project](/docs/home/getting-started/vocabulary#project), enforcing a standardized process and ensuring they are following best practices.
To learn more, read [Release pipelines](/docs/home/releases/release-pipelines).
### Release policy
A `release policy` is a prescribed set of default configurations or preferences that define how releases are created within a given scope. Release policies standardize release behavior so that engineers do not need to configure all release settings individually. For example, a release policy can specify that all [guarded releases](/docs/home/getting-started/vocabulary#guarded-release-or-guarded-rollout) in a production environment use automatic rollbacks and require a minimum sample size of 100.
To learn more, read [Release policies](/docs/home/releases/release-policies).
### Role
A `role` is a description of the access that a [member](/docs/home/getting-started/vocabulary#member) or [team](/docs/home/getting-started/vocabulary#team) has within LaunchDarkly. Every LaunchDarkly account comes with several built-in [base roles](/docs/home/getting-started/vocabulary#base-role), including Reader, Writer, Admin, and Owner.
Customers on select plans additionally have:
 * access to a No access base role.
 * access to several [organization roles](/docs/home/getting-started/vocabulary#organization-role) and [project roles](/docs/home/getting-started/vocabulary#project-role) provided by LaunchDarkly. These provide different sets of permissions that are commonly grouped together, designed around typical personas. For example, LaunchDarkly provides a Developer project role that can perform all flag actions within projects they are assigned, and a Contributor project role that can make changes to flag status but cannot perform destructive actions on it.
 * the ability to create their own roles, sometimes called custom roles. When you create your own role, you define the access using a set of statements called a policy.
Every member must have at least one role assigned to them, either directly or through a team. This is true even if the role explicitly prohibits them from accessing any information within LaunchDarkly.
To learn more, read [Roles](/docs/home/account/roles) and [Member role concepts](/docs/home/account/roles/role-concepts).
### Role attribute
A `role attribute` is a key that you may use to parameterize a [role](/docs/home/getting-started/vocabulary#role). When you define a role, you can optionally specify a [role scope](/docs/home/getting-started/vocabulary#role-scope) and corresponding role attributes. Some [preset roles](/docs/home/getting-started/vocabulary#preset-roles) also include role scope.
For example, suppose Member A should have access to all actions on flags in Project A, and Member B should have access to all actions on flags in Project B. You can create one role with access to all actions on flags, and set a role scope of project, using a role attribute of `developerProjectKey`.
When you assign this role to Member A, you can specify “Project A” as the value of the `developerProjectKey` role attribute. This gives Member A access to all actions on flags in Project A. When you assign the same role to Member B, you can specify “Project B” as the value of the `developerProjectKey` role attribute to give Member B access to all actions on flags in Project B.
`roleAttributes` cannot be passed through SAML assertions for single sign-on. For SAML-based SSO, LaunchDarkly supports mapping only the `role`, `customRole`, and `teamKey` attributes.
To learn more, read [Member role concepts](/docs/home/account/roles/role-concepts).
### Role scope
A `role scope` is a [resource type](/docs/home/account/roles/role-resources) by which a [role](/docs/home/getting-started/vocabulary#role) may be parameterized. When you define a role, you can optionally specify a role scope and the parameter, which is called a [role attribute](/docs/home/getting-started/vocabulary#role-attribute). Some [preset roles](/docs/home/getting-started/vocabulary#preset-roles) also include role scope.
For example, suppose Member A should have access to all actions on flags in Project A, and Member B should have access to all actions on flags in Project B. You can create one role with access to all actions on flags, and set a role scope of project. Then, you can specify Project A when you assign this role to Member A, and specify Project B when you assign this role to Member B.
To learn more, read [Member role concepts](/docs/home/account/roles/role-concepts).
### Rule
A `rule` or `targeting rule` is a description of which [contexts](/docs/home/getting-started/vocabulary#context) should be included for a given outcome. In [flags](/docs/home/getting-started/vocabulary#flag), targeting rules determine which flag variations your application should serve to which contexts. In [segments](/docs/home/getting-started/vocabulary#segment), targeting rules determine which contexts are part of the segment. In [AI Configs](/docs/home/getting-started/vocabulary#ai-config), targeting rules determine which variations your application should serve to which contexts.
Targeting rules can have one or more conditions. Each condition has three parts:
 * A context kind and attribute, which defines the scope of the condition’s impact, such as only targeting an email address for the selected context kind.
 * An operator, which sets differentiating characteristics of the attribute, such as limiting the condition to emails that end with certain extensions.
 * A value, which identifies the attribute by a value you specify, such as `.edu`.
To learn more, read [Target with flags](/docs/home/flags/target), [Targeting contexts in rule-based and smaller list-based segments](/docs/home/flags/rule-based-segments), [Targeting contexts in larger list-based segments](/docs/home/flags/list-based-segments), and [Target with AI Configs](/docs/home/ai-configs/target).
## S
### Sample ratio mismatch (SRM)
A `sample ratio mismatch` (SRM) occurs when the distribution of traffic across different [variations](/docs/home/getting-started/vocabulary#variation) deviates from the expected allocation. An SRM can bias or invalidate results. SRMs are most commonly caused by outdated [SDKs](/docs/home/getting-started/vocabulary#sdk) or issues with SDK or [metric](/docs/home/getting-started/vocabulary#metric) implementation.
To learn more, read [Analyzing experiments](/docs/home/experimentation/analyze).
### Saved search
A `saved search` is a set of search filters stored in the LaunchDarkly UI. Saved searches return specific sessions or errors and can be shared with others through URLs. Saved searches are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Search specification](/docs/home/observability/search).
### SDK
The `LaunchDarkly SDK` is the software development kit that you use to integrate LaunchDarkly with your application’s code.
We provide more than two dozen LaunchDarkly SDKs, in different languages and frameworks. Our `client-side SDKs` are designed for single-user desktop, mobile, and embedded applications. They are intended to be used in a potentially less secure environment, such as a personal computer or mobile device. Our `server-side SDKs` are designed for multi-user systems. They are intended to be used in a trusted environment, such as inside a corporate network or on a web server.
When your application starts, your code should initialize the LaunchDarkly SDK you’re working with. When a customer encounters a feature flag in your application, your code should use the SDK to evaluate the feature flag and retrieve the appropriate flag variation for that customer.
To learn more, read [Setting up an SDK](/docs/home/getting-started/setting-up) and [Choosing an SDK type](/docs/sdk/concepts/client-side-server-side). For more information about the differences between the LaunchDarkly SDK and the LaunchDarkly REST API, read [Comparing LaunchDarkly’s SDKs and REST API](/docs/guides/api/comparing-sdk-rest-api).
### Search specification
A `search specification` defines the syntax and operators available to query and filter observability data. It supports comparisons, logical operators, regex, wildcards, and saved searches across sessions, errors, logs, and traces. The search specification is limited to observability and does not apply to feature flags, rollouts, or experiments.
To learn more, read [Search specification](/docs/home/observability/search).
### Segment
A `segment` is a list of contexts that you can use to manage flag [targeting](/docs/home/getting-started/vocabulary#target) behavior in bulk. Segments are useful for keeping groups of contexts, like `beta-users` or `enterprise-customers`, up to date. They are [environment-specific](/docs/home/getting-started/vocabulary#environment).
LaunchDarkly supports:
 * rule-based segments, which let you target groups of contexts individually or by attributes,
 * list-based segments, which let you target individual contexts or uploaded lists of contexts, and
 * synced segments, which let you target groups of contexts backed by an external data store.
To learn more, read [Segments](/docs/home/flags/segments).
Segment is also the name of a third-party software application that collects and integrates customer data across tools. LaunchDarkly integrates with Segment in the following ways:
 * You can use Segment as a destination for LaunchDarkly’s Data Export feature. To learn more, read [Segment](/docs/integrations/data-export/segment).
 * You can use Segment as a source for metric events. To learn more, read [Segment for metrics](/docs/integrations/metric-segment).
 * Segment Audiences is one of several tools you can use to create synced segments. To learn more, read [Segments synced from external tools](/docs/home/flags/synced-segments).
### Session
A `session` is a recorded period of interaction between an end user and your application. Sessions are captured by the session replay SDK plugin and may include events, errors, logs, and traces. Sessions are the foundation for replays and can be filtered by attributes such as browser, device, or location. Sessions are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Session replay](/docs/home/observability/session-replay).
### Session replay
A `session replay` is a recorded exploration of an end user’s session in your application. LaunchDarkly records user interactions so you can play them back to understand behavior, diagnose issues, and improve user experience. Session replays are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Session replay](/docs/home/observability/session-replay).
### Significance level
The `significance level` of a frequentist [experiment](/docs/home/getting-started/vocabulary#experiment) is the configured allowable rate of false positives for an experiment. This is sometimes called the “false positive rate.” The significance level is usually represented by the Greek letter “alpha.”
### Span
A `span` is the fundamental unit of a [trace](/docs/home/getting-started/vocabulary#trace). A span represents a single operation or step in a trace and includes attributes such as span name, duration, parent span ID, and related context. Spans are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Traces](/docs/home/observability/traces).
### Standard metric group
A `standard metric group` is a reusable set of metrics you can use with [feature change experiments](/docs/home/getting-started/vocabulary#feature-change-experiment) to standardize metrics across multiple experiments.
To learn more, read [Metric groups](/docs/home/metrics/metric-groups).
### Statistical significance
In frequentist [experiments](/docs/home/getting-started/vocabulary#experiment), `statistical significance` indicates the likelihood that the observed relationship or effect in the data is not due to random chance. When an experiment result is statistically significant, it means the observed effect is unlikely to have occurred purely by random variation.
### String flag
A `string flag` is a feature flag type with variations that hold simple configuration values. They are multivariate flags, meaning they can have more than two variations.
String flags can be used for things like switching between different versions of header text, URLs, or other configurations.
To learn more, read [Flag types](/docs/home/flags/types).
## T
### Target
To `target` (verb) is to specify that specific [contexts](/docs/home/getting-started/vocabulary#context) that encounter [feature flags](/docs/home/getting-started/vocabulary#flag) or [AI Configs](/docs/home/getting-started/vocabulary#ai-config) in your application should receive a specific variation of that resource. A `target` (noun) is an individual context or a set of contexts described by a targeting [rule](/docs/home/getting-started/vocabulary#rule).
To learn more, read [Target with flags](/docs/home/flags/target), [Targeting contexts in rule-based and smaller list-based segments](/docs/home/flags/rule-based-segments), [Targeting contexts in larger list-based segments](/docs/home/flags/list-based-segments), and [Target with AI Configs](/docs/home/ai-configs/target).
### Team
A `team` is a group of members in your LaunchDarkly account. To learn more, read [Teams](/docs/home/account/teams).
### Threshold
In Bayesian [experiments](/docs/home/getting-started/vocabulary#experiment), the `threshold` indicates how confident you’d like to be in the experiment’s results before making a decision. A standard threshold is 90%, which means that you want an experiment [treatment variation](/docs/home/getting-started/vocabulary#treatment-variation) to have a [probability to beat the control](/docs/home/getting-started/vocabulary#probability-to-beat-control) variation of at least 90% before declaring it a [winning variation](/docs/home/getting-started/vocabulary#winning-variation).
If you require higher confidence, such as 95% or 99%, it will typically take more time to gather enough data to reach that level of certainty. To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results).
### Total value
In [experiment](/docs/home/getting-started/vocabulary#experiment) results, the `total value` column displays the sum total of all the numbers returned by a numeric metric.
To learn more, read [Bayesian experiment results](/docs/home/experimentation/bayesian-results) and [Frequentist experiment results](/docs/home/experimentation/frequentist-results).
### Trace
A `trace` is a record of activity across spans in your application. Traces show how requests flow through services and components. Each trace consists of spans, which include attributes such as span name, duration, and related context. Traces are limited to observability and do not apply to feature flags, rollouts, or experiments.
To learn more, read [Traces](/docs/home/observability/traces).
### Traffic allocation
An [experiment’s](/docs/home/getting-started/vocabulary#experiment) `traffic allocation` is the amount of [contexts](/docs/home/getting-started/vocabulary#context) you assign to each flag [variation](/docs/home/getting-started/vocabulary#variation) you’re experimenting on.
To learn more, read [Allocating experiment audiences](/docs/home/experimentation/allocation).
### Treatment variation
The `treatment variation` is the new flag [variation](/docs/home/getting-started/vocabulary#variation) being released during a [progressive release](/docs/home/getting-started/vocabulary##progressive-release-or-progressive-rollout) or [experiment](/docs/home/getting-started/vocabulary#experiment). It is compared against the [control variation](/docs/home/getting-started/vocabulary#control-variation) to determine if the treatment has a negative or positive effect on the [metric](/docs/home/getting-started/vocabulary#metric) being measured. If the treatment variation’s metrics perform worse than the control, LaunchDarkly identifies a [regression](/docs/home/getting-started/vocabulary#regression). The treatment variation represents the change being introduced in the progressive release.
To learn more, read [Experimentation](/docs/home/experimentation).
## U
### Unit aggregation method
The `unit aggregation method` for a [metric](/docs/home/getting-started/vocabulary#metric) is the mathematical method you want to aggregate [event](/docs/home/getting-started/vocabulary#event) values by for the metric’s results. You can aggregate either by sum or by average.
To learn more, read [Unit aggregation method](/docs/home/metrics/metric-analysis#unit-aggregation-method).
### User
Previously, a `user` was the only way to refer to an entity that encountered feature flags in your product.
Newer versions of the LaunchDarkly SDKs replace users with [contexts](/docs/home/getting-started/vocabulary#context). Contexts are a more powerful and flexible way of referring to the people, services, machines, or other resources that encounter feature flags in your product. A user is just one [kind](/docs/home/getting-started/vocabulary#context-kind) of context.
People who are logged in to the LaunchDarkly user interface are called [members](/docs/home/getting-started/vocabulary#member).
## V
### Variation
A `variation` is a description of a possible value that a [flag](/docs/home/getting-started/vocabulary#flag) or [AI Config](/docs/home/getting-started/vocabulary#ai-config) can have.
For flags, each variation must contain the possible flag value. It may also contain a name and description. For AI Configs, each variation must contain a model and one or more messages. It must also have a name.
Both flags and AI Configs share variations across environments within a project. However, they can have different states, [targets](/docs/home/getting-started/vocabulary#target), and [rules](/docs/home/getting-started/vocabulary#rule) in each environment.
When you create a flag, you must decide whether it is a boolean flag or a multivariate flag. Boolean flags have exactly two variations, with values of “true” and “false.” Multivariate flags can have more than two variations. Each of the variations must have a value of the same type, for example, a string.
AI Config variations all have the same structure: a name, model configuration, and one or more messages.
To learn more, read [Creating flag variations](/docs/home/flags/variations) and [Create and manage AI Config variations](/docs/home/ai-configs/create-variation).
In [migration flags](/docs/home/getting-started/vocabulary#migration-flag), variations are built-in and cannot be edited because they are linked to the migration’s stages. To learn more, read [Targeting with migration flags](/docs/home/flags/target).
### View
A `view` is a resource that logically groups flags within a [project](/docs/home/getting-started/vocabulary#project). You can create multiple views within each project. For example, you can use views to group flags according to the teams in your organization and the features they work on. A given flag can be linked to more than one view.
Views let you restrict access to sets of flags. For example, you can create a role that only allows access to flags in certain views.
To learn more, read [Views](/docs/home/account/views). For an example of using a view in a role policy, read [Example: View-specific permissions](/docs/home/account/roles/example-roles#example-view-specific-permissions).
## W
### Winning variation
An [experiment’s](/docs/home/getting-started/vocabulary#experiment) `winning variation` is the [variation](/docs/home/getting-started/vocabulary#variation) that performed the best out of all the variations tested.
For [Bayesian](/docs/home/getting-started/vocabulary#bayesian-statistics) experiments, the winning variation is the variation with the highest [probability to be best](/docs/home/getting-started/vocabulary#probability-to-be-best) that exceeds the Bayesian [threshold](/docs/home/getting-started/vocabulary#threshold) you set when you created the experiment. If a Bayesian experiment has collected enough data to determine a winning variation, and that winning variation is not the control, then the winning variation is highlighted in green in its results chart.
For [frequentist](/docs/home/getting-started/vocabulary#frequentist-statistics) experiments, every experiment iteration displays each variation’s [p-value](/docs/home/getting-started/vocabulary#p-value). The variation with the highlighted p-value is the winning variation.
To learn more, read [Analyzing experiments](/docs/home/experimentation/analyze).
[![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)](/docs/home)
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs