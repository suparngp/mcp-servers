`/`
[Product docs](/docs/home)[Guides](/docs/guides)[SDKs](/docs/sdk)[Integrations](/docs/integrations)[API docs](/docs/api)[Tutorials](/docs/tutorials)[Flagship Blog](/docs/blog)
 * Getting started
 * [Overview](/docs/home)
 * [Launch Insights](/docs/home/getting-started/launch-insights)
 * [LaunchDarkly architecture](/docs/home/getting-started/architecture)
 * [LaunchDarkly vocabulary](/docs/home/getting-started/vocabulary)
 * Feature flags and AI Configs
 * [Create flags](/docs/home/flags/create)
 * [Target with flags](/docs/home/flags/target)
 * [Flag templates](/docs/home/flags/templates)
 * [Manage flags](/docs/home/flags/manage)
 * [Code references](/docs/home/flags/code-references)
 * [AI Configs](/docs/home/ai-configs)
 * [Contexts](/docs/home/flags/contexts)
 * [Segments](/docs/home/flags/segments)
 * Releases
 * [Releasing features with LaunchDarkly](/docs/home/releases/releasing)
 * [Release policies](/docs/home/releases/release-policies)
 * [Percentage rollouts](/docs/home/releases/percentage-rollouts)
 * [Progressive rollouts](/docs/home/releases/progressive-rollouts)
 * [Guarded rollouts](/docs/home/releases/guarded-rollouts)
 * [Feature monitoring](/docs/home/releases/feature-monitoring)
 * [Release pipelines](/docs/home/releases/release-pipelines)
 * [Engineering insights](/docs/home/releases/eng-insights)
 * [Release management tools](/docs/home/releases/release-management)
 * [Applications and app versions](/docs/home/releases/apps-and-app-versions)
 * [Change history](/docs/home/releases/change-history)
 * Observability
 * [Observability](/docs/home/observability)
 * [Session replay](/docs/home/observability/session-replay)
 * [Error monitoring](/docs/home/observability/errors)
 * [Logs](/docs/home/observability/logs)
 * [Traces](/docs/home/observability/traces)
 * [LLM observability](/docs/home/observability/llm-observability)
 * [Alerts](/docs/home/observability/alerts)
 * [Dashboards](/docs/home/observability/dashboards)
 * [Search specification](/docs/home/observability/search)
 * [Observability settings](/docs/home/observability/settings)
 * [Vega](/docs/home/observability/vega)
 * Product analytics
 * [Product analytics](/docs/home/product-analytics)
 * [Setting up product analytics](/docs/home/product-analytics/setup)
 * [Using product analytics charts](/docs/home/product-analytics/chart)
 * Experimentation
 * [Experimentation](/docs/home/experimentation)
 * [Creating experiments](/docs/home/experimentation/create)
 * [Managing experiments](/docs/home/experimentation/manage)
 * [Analyzing experiments](/docs/home/experimentation/analyze)
 * [Experimentation and metric events](/docs/home/experimentation/events)
 * [Multi-armed bandits](/docs/home/multi-armed-bandits)
 * [Holdouts](/docs/home/holdouts)
 * Metrics
 * [Metrics](/docs/home/metrics)
 * [Metric groups](/docs/home/metrics/metric-groups)
 * [Autogenerated metrics](/docs/home/metrics/autogen-metrics)
 * [Metric impact](/docs/home/metrics/metric-impact)
 * [Metric events](/docs/home/metrics/metric-events)
 * Warehouse native
 * [Warehouse native Experimentation](/docs/home/warehouse-native)
 * [Warehouse native metrics](/docs/home/warehouse-native/metrics)
 * [Creating warehouse native experiments](/docs/home/warehouse-native/creating)
 * Infrastructure
 * [Connect apps and services to LaunchDarkly](/docs/home/infrastructure/apps)
 * [LaunchDarkly in China and Pakistan](/docs/home/infrastructure/china)
 * [LaunchDarkly in the European Union (EU)](/docs/home/infrastructure/eu)
 * [LaunchDarkly in federal environments](/docs/home/infrastructure/federal)
 * [Public IP list](/docs/home/infrastructure/ip-list)
 * Your account
 * [Projects](/docs/home/account/project)
 * [Views](/docs/home/account/views)
 * [Environments](/docs/home/account/environment)
 * [Tags](/docs/home/account/tags)
 * [Teams](/docs/home/account/teams)
 * [Members](/docs/home/account/members)
 * [Roles](/docs/home/account/roles)
 * [Account security](/docs/home/account/secure)
 * [Billing and usage](/docs/home/account/billing)
 * [Changelog](/docs/home/changelog)
[Sign in](/)[Sign up](https://app.launchdarkly.com/signup)
On this page
 * [Overview](#overview)
 * [Prerequisites](#prerequisites)
 * [Create flags](#create-flags)
 * [Create metrics](#create-metrics)
 * [Build experiments](#build-experiments)
 * [Turn on feature flags](#turn-on-feature-flags)
 * [Start experiment iterations](#start-experiment-iterations)
## Overview
This topic explains how to set up and configure an experiment in LaunchDarkly that uses metric events from your own data warehouse.
Before you create a warehouse native experiment, you must enable [Warehouse Data Export](/docs/integrations/data-export/warehouse) and configure the warehouse native Experimentation app in your external warehouse for your LaunchDarkly account.
Configuring a warehouse native experiment requires several steps:
 1. [Creating the flag and its variations](/docs/home/warehouse-native/creating#create-flags),
 2. [Creating one or more warehouse-native metrics](/docs/home/warehouse-native/creating#create-metrics),
 3. [Building the experiment](/docs/home/warehouse-native/creating#build-experiments),
 4. [Turning on the feature flag](/docs/home/warehouse-native/creating#turn-on-feature-flags), and
 5. [Starting an iteration](/docs/home/warehouse-native/creating#start-experiment-iterations).
These steps are explained in detail below.
### Prerequisites
Before you build a warehouse native experiment, you must:
 * enable [Warehouse Data Export](/docs/integrations/data-export/warehouse) in your LaunchDarkly account for the specific warehouse you’re using
 * if you’re using Snowflake, configure the Snowflake native Experimentation app for your LaunchDarkly account
 * create a [warehouse-native metric](/docs/home/warehouse-native/metrics)
 * understand [randomization units](/docs/home/experimentation/randomization)
## Create flags
Before you begin an experiment, create a flag the variations you plan to test the performance of. You do not need to toggle on the flag before you create an experiment, but you do have to toggle on the flag before you [start an experiment iteration](/docs/home/warehouse-native/creating#start-experiment-iterations).
You cannot run a warehouse native experiment on a flag if:
 * the flag has an active [guarded rollout](/docs/home/releases/guarded-rollouts)
 * the flag has an active [progressive rollout](/docs/home/releases/progressive-rollouts)
 * the flag is already in a running experiment
 * the flag is a [migration flag](/docs/home/flags/migration)
You can build multiple warehouse native experiments on the same flag, but you can run only one of those experiments at a time.
To learn more, read [Creating new flags](/docs/home/flags/new) and [Creating flag variations](/docs/home/flags/variations).
## Create metrics
##### Warehouse native experiments do not work with all metric types
Warehouse native experiments can use only warehouse-native metrics. Be sure you have configured at least one warehouse-native metric before you create a warehouse native experiment.
Warehouse native experiments can use only [warehouse-native metrics](/docs/home/warehouse-native/metrics). Warehouse native metrics must:
 * be either [custom conversion binary](/docs/home/metrics/custom), [custom conversion count](/docs/home/metrics/custom-count), or [custom numeric](/docs/home/metrics/custom-numeric)
 * use the [“average” analysis method](/docs/home/metrics/metric-analysis#analysis-method)
Warehouse native experiments cannot use:
 * [clicked or tapped metrics](/docs/home/metrics/click)
 * [page viewed metrics](/docs/home/metrics/pageview)
 * [metric groups](/docs/home/metrics/metric-groups)
 * LaunchDarkly hosted metrics
 * a [percentile analysis method](/docs/home/metrics/metric-analysis#analysis-method)
## Build experiments
To build an experiment:
 1. Click **Create** and choose **Experiment**. The “Create experiment” dialog appears.
 2. Enter an experiment **Name**.
 3. Enter a **Hypothesis**.
 4. Click **Create experiment**. The experiment **Design** tab appears.
 5. Select the **Snowflake native** experiment type.
 6. Choose a context kind to **Randomize by**.
 7. Select one or more **Metrics**. The metrics you select must be [warehouse-native](/docs/home/warehouse-native/metrics).
 * (Optional) If you have added multiple metrics and want to change the primary metric, hover on the metric name and click the **crown** icon.
 * Click **Create** to create and use a new warehouse native metric.
 8. Choose a **Flag** to use in the experiment.
 * Click **Create flag** to create and use a new flag.
 9. Choose a targeting rule for the **Experiment audience**.
 * If you want to restrict your experiment audience to only contexts with certain attributes, create a targeting rule on the flag you include in the experiment and run the experiment on that rule.
 * If you don’t want to restrict the audience for your experiment, run the experiment on the default rule. If the flag or doesn’t have any targeting rules, the default rule will be the only option.
![The "Audience targeting" section with the default rule chosen.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/39503b0adb7e876fa64cde29342d11ab9b06de5c05a2218fe8c06f7057729d77/assets/images/__toPlaywright_newIA/experiment-builder-define-audience.png)
The "Audience targeting" section with the default rule chosen.
 1. (Optional) If you want to [exclude contexts in this experiment](/docs/home/experimentation/mutually-exclusive) from certain other experiments, click **Add experiment to exclusion layer** and select a layer.
###### Expand layer options
A [layer](/docs/home/experimentation/mutually-exclusive) is a set of experiments that cannot share traffic with each other. All of the experiments within a layer are mutually exclusive, which means that if a context is included in one experiment, LaunchDarkly will exclude it from any other experiments in the same layer.
To add the experiment to an existing layer:
 1. Click **Select layer**.
 2. Search for and choose the layer you want to add the experiment to.
 3. Enter a **Reservation** amount. This is the percentage of the contexts within this layer you want LaunchDarkly to include in this experiment.
 4. Click **Save layer**.
If you need to create a new layer:
 1. Click **Create layer**.
 2. Add a **Name** and **Description**.
 3. Click **Create layer**.
 4. Enter a **Reservation** amount. This is the percentage of the contexts within this layer you want LaunchDarkly to include in this experiment.
 5. Click **Save layer**.
 1. Choose the **Variation served to users outside this experiment**. Contexts that match the selected targeting rule but are not in the experiment will receive this variation.
 2. Select the **Sample size** for the experiment. This is the percentage of all of the contexts that match the experiment’s targeting rule that you want to include in the experiment.
 3. (Optional) Click **Advanced** to edit variation reassignment. For most experiments, we recommend leaving this option on its default setting. To learn more, read [Carryover bias and variation reassignment](/docs/home/experimentation/carryover).
 4. (Optional) Click **Edit** to update the variation split for contexts that are in the experiment.
 * You can **Split equally** between variations, or assign a higher percentage of contexts to some variations than others.
 * Click **Save audience split**.
 5. Select a variation to serve as the **Control**.
 6. Select a **Statistical approach** of Bayesian or frequentist.
 * If you selected a statistical approach of Bayesian, select a preset or **Custom** success threshold.
 * If you selected a statistical approach of frequentist, select:
 * a **Significance level**.
 * a one-sided or two-sided **Direction of hypothesis test**.
###### Expand statistical approach options
You can select a statistical approach of **Bayesian** or **Frequentist**. Each approach includes one or more analysis options.
We recommend **Bayesian** when you have a small sample size of less than a thousand contexts, and we recommend **Frequentist** when you have a larger sample size of a thousand or more.
The Bayesian options include:
 * **Threshold** :
 * **90%** probability to beat control is the standard success threshold, but you can raise the threshold to **95%** or **99%** if you want to be more confident in your experiment results.
 * You can lower the threshold to less than 90% using the **Custom** option. We recommend a lower threshold only when you are experimenting on non-critical parts of your app and are less concerned with determining a clear winning variation.
The frequentist options include:
 1. **Significance level** :
 * **0.05** p-value is the standard significance level, but you can lower the level to **0.01** or raise the level to **0.10** , depending on whether you need to be more or less confident in your results. A lower significance level means that you can be more confident in your winning variation.
 * You can raise the significance level to more than 0.10 using the **Custom** option. We recommend a higher significance level only when you are experimenting on non-critical parts of your app and are less concerned with determining a clear winning variation.
 2. **Direction of hypothesis test** :
 * **Two-sided** : We recommend two-sided when you’re unsure about whether the difference between the control and the treatment variations will be negative or positive, and want to look for indications of statistical significance in both directions.
 * **One-sided** : We recommend one-sided when you feel confident that the difference between the control and treatment variations will be either negative or positive, and want to look for indications of statistical significance only in one direction.
 3. (Optional) Select a [Multiple comparisons correction](/docs/guides/statistical-methodology/mcc) option.
 * Select **Apply across treatments** to correct for additional comparisons from multiple treatments
 * Select **Apply across metrics** to correct for additional comparisons across multiple metrics
 * Select **Apply across both metrics and treatments** to correct for additional comparisons from multiple metrics and multiple treatments
To learn more, read [Bayesian versus frequentist statistics](/docs/guides/experimentation/bayesian-frequentist).
 1. (Optional) If you want to include the experiment in a [holdout](/docs/home/holdouts), click **Advanced** , then select a **Holdout** name.
##### Experiments cannot be in a holdout and in a layer at the same time
Experiments can either be in a holdout or in a layer, but not both. If you added the experiment to a layer, you will not see the option to add it to a holdout.
 1. (Optional) If you want to be able to [filter your experiment results by attribute](/docs/home/experimentation/filters), click **Advanced** , then select up to five context attributes to filter results by.
 2. (Optional) Click **Add tags** in the right sidebar to add new or existing tags to the experiment.
 3. Scroll to the top of the page and click **Save**.
If needed, you can save your in-progress experiment design to finish later. To save your design, click **Save** at the top of the creation screen. Your in-progress experiment design is saved and appears on the **Experiments** list. To finish building the experiment, click on the experiment’s name and continue editing.
After you have created your experiment, the next step is to toggle on the flag. Then, you can start an iteration.
![](https://fern-image-hosting.s3.us-east-1.amazonaws.com/launchdarkly/openapi-logo.svg)
You can also use the REST API: [Create experiment](https://apidocs.launchdarkly.com/tag/Experiments#operation/createExperiment)
## Turn on feature flags
For an experiment to begin recording data, the flag used in the experiment must be on. To learn how, read [Turning flags on and off](/docs/home/flags/toggle).
## Start experiment iterations
After you create an experiment and toggle on the flag, you can start an experiment iteration in one or more environments.
To start an experiment iteration:
 1. Navigate to the **Experiments** list.
 2. Click on the environment section containing the experiment you want to start.
 * If the environment you need isn’t visible, click the **+** next to the list of environment sections. Search for the environment you want, and select it from the list.
![The environment selection menu.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/34b4f0b3517652a1d52892a144f582c4acfeb5b49bfaf2ffee688fca284f3ce6/assets/images/auto/environment-selection.auto.png)
The environment selection menu.
 1. Click on the name of the experiment you want to start an iteration for. The **Design** tab appears.
 2. Click **Start**.
 3. Repeat steps 1-4 for each environment you want to start an iteration in.
![An experiment with the "Start" button called out.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/4856586600e776039dac53473c7f3f577efab2d83cf765772a56660a4f528659/assets/images/__toPlaywright_newIA/experiment-details-start-callout.png)
An experiment with the "Start" button called out.
Experiment iterations allow you to record experiments in individual blocks of time. To ensure accurate experiment results, when you make changes that impact an experiment, LaunchDarkly starts a new iteration of the experiment.
To learn more about starting and stopping iterations, read [Starting and stopping experiment iterations](/docs/home/experimentation/start-stop-exp)..
![](https://fern-image-hosting.s3.us-east-1.amazonaws.com/launchdarkly/openapi-logo.svg)
You can also use the REST API: [Create iteration](https://apidocs.launchdarkly.com/tag/Experiments#operation/createIteration)
[![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)](/docs/home)
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs