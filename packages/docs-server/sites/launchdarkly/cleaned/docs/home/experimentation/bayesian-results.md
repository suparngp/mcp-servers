`/`
[Product docs](/docs/home)[Guides](/docs/guides)[SDKs](/docs/sdk)[Integrations](/docs/integrations)[API docs](/docs/api)[Tutorials](/docs/tutorials)[Flagship Blog](/docs/blog)
 * Getting started
 * [Overview](/docs/home)
 * [Launch Insights](/docs/home/getting-started/launch-insights)
 * [LaunchDarkly architecture](/docs/home/getting-started/architecture)
 * [LaunchDarkly vocabulary](/docs/home/getting-started/vocabulary)
 * Feature flags and AI Configs
 * [Create flags](/docs/home/flags/create)
 * [Target with flags](/docs/home/flags/target)
 * [Flag templates](/docs/home/flags/templates)
 * [Manage flags](/docs/home/flags/manage)
 * [Code references](/docs/home/flags/code-references)
 * [AI Configs](/docs/home/ai-configs)
 * [Contexts](/docs/home/flags/contexts)
 * [Segments](/docs/home/flags/segments)
 * Releases
 * [Releasing features with LaunchDarkly](/docs/home/releases/releasing)
 * [Release policies](/docs/home/releases/release-policies)
 * [Percentage rollouts](/docs/home/releases/percentage-rollouts)
 * [Progressive rollouts](/docs/home/releases/progressive-rollouts)
 * [Guarded rollouts](/docs/home/releases/guarded-rollouts)
 * [Feature monitoring](/docs/home/releases/feature-monitoring)
 * [Release pipelines](/docs/home/releases/release-pipelines)
 * [Engineering insights](/docs/home/releases/eng-insights)
 * [Release management tools](/docs/home/releases/release-management)
 * [Applications and app versions](/docs/home/releases/apps-and-app-versions)
 * [Change history](/docs/home/releases/change-history)
 * Observability
 * [Observability](/docs/home/observability)
 * [Session replay](/docs/home/observability/session-replay)
 * [Error monitoring](/docs/home/observability/errors)
 * [Logs](/docs/home/observability/logs)
 * [Traces](/docs/home/observability/traces)
 * [LLM observability](/docs/home/observability/llm-observability)
 * [Alerts](/docs/home/observability/alerts)
 * [Dashboards](/docs/home/observability/dashboards)
 * [Search specification](/docs/home/observability/search)
 * [Observability settings](/docs/home/observability/settings)
 * [Vega](/docs/home/observability/vega)
 * Product analytics
 * [Product analytics](/docs/home/product-analytics)
 * [Setting up product analytics](/docs/home/product-analytics/setup)
 * [Using product analytics charts](/docs/home/product-analytics/chart)
 * Experimentation
 * [Experimentation](/docs/home/experimentation)
 * [Creating experiments](/docs/home/experimentation/create)
 * [Managing experiments](/docs/home/experimentation/manage)
 * [Analyzing experiments](/docs/home/experimentation/analyze)
 * [Experimentation and metric events](/docs/home/experimentation/events)
 * [Multi-armed bandits](/docs/home/multi-armed-bandits)
 * [Holdouts](/docs/home/holdouts)
 * Metrics
 * [Metrics](/docs/home/metrics)
 * [Metric groups](/docs/home/metrics/metric-groups)
 * [Autogenerated metrics](/docs/home/metrics/autogen-metrics)
 * [Metric impact](/docs/home/metrics/metric-impact)
 * [Metric events](/docs/home/metrics/metric-events)
 * Warehouse native
 * [Warehouse native Experimentation](/docs/home/warehouse-native)
 * [Warehouse native metrics](/docs/home/warehouse-native/metrics)
 * [Creating warehouse native experiments](/docs/home/warehouse-native/creating)
 * Infrastructure
 * [Connect apps and services to LaunchDarkly](/docs/home/infrastructure/apps)
 * [LaunchDarkly in China and Pakistan](/docs/home/infrastructure/china)
 * [LaunchDarkly in the European Union (EU)](/docs/home/infrastructure/eu)
 * [LaunchDarkly in federal environments](/docs/home/infrastructure/federal)
 * [Public IP list](/docs/home/infrastructure/ip-list)
 * Your account
 * [Projects](/docs/home/account/project)
 * [Views](/docs/home/account/views)
 * [Environments](/docs/home/account/environment)
 * [Tags](/docs/home/account/tags)
 * [Teams](/docs/home/account/teams)
 * [Members](/docs/home/account/members)
 * [Roles](/docs/home/account/roles)
 * [Account security](/docs/home/account/secure)
 * [Billing and usage](/docs/home/account/billing)
 * [Changelog](/docs/home/changelog)
[Sign in](/)[Sign up](https://app.launchdarkly.com/signup)
On this page
 * [Overview](#overview)
 * [The winning variation](#the-winning-variation)
 * [Ship the winning variation](#ship-the-winning-variation)
 * [Sharing options](#sharing-options)
 * [Visualization options](#visualization-options)
 * [Probability density](#probability-density)
 * [Relative difference](#relative-difference)
 * [Arm averages](#arm-averages)
 * [Filter options](#filter-options)
 * [Metrics filter](#metrics-filter)
 * [Variations filter](#variations-filter)
 * [Attributes filter](#attributes-filter)
 * [Results chart data](#results-chart-data)
 * [Graph](#graph)
 * [Probability to be best](#probability-to-be-best)
 * [Probability to beat control](#probability-to-beat-control)
 * [Probability to beat control in funnel optimization experiments](#probability-to-beat-control-in-funnel-optimization-experiments)
 * [Relative difference](#relative-difference-1)
 * [Expected loss](#expected-loss)
 * [Exposures](#exposures)
 * [Metric-specific results](#metric-specific-results)
 * [Binary conversion metrics](#binary-conversion-metrics)
 * [Conversion rate (posterior mean)](#conversion-rate-posterior-mean)
 * [Conversions](#conversions)
 * [Count conversion metrics](#count-conversion-metrics)
 * [Posterior mean](#posterior-mean)
 * [Total value](#total-value)
 * [Numeric metrics](#numeric-metrics)
 * [Posterior mean](#posterior-mean-1)
 * [Total value](#total-value-1)
## Overview
This topic explains how to read the results of a Bayesian experiment.
![A Bayesian experiment's results table.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/c835f3f2f3515c5e2ea41cb2f61422d6e4358f73e594a983fd10041bbdbabf5a/assets/images/__LD_UI_no_test/experiment-results-table-bayesian.png)
A Bayesian experiment's results table.
## The winning variation
For experiments using Bayesian statistics, the winning variation is the variation with the highest [probability to be best](/docs/home/getting-started/vocabulary#probability-to-be-best) that exceeds the Bayesian [threshold](/docs/home/getting-started/vocabulary#threshold) you set when you created the experiment.
If a Bayesian experiment has collected enough data to determine a winning variation, and the winning variation is not the control, then the winning variation is highlighted in green in its results chart.
### Ship the winning variation
To stop an experiment and ship the winning variation:
 1. Navigate to the experiment’s **Results** tab.
 2. Click **Stop**. The “Ship” menu appears.
 3. Select the winning variation to ship to all contexts that match the flag’s targeting rule. A “Stop experiment” dialog appears.
 4. Enter a **Reason for stopping**.
 5. Click **Stop experiment**.
Bayesian experiments also display a **Ship it** button if the experiment has enough data to determine a winning variation, and that winning variation is not the control. You can use this option to stop the experiment and ship the winning variation.
## Sharing options
To download and share a PDF version of the experiment **Results** tab, click **Download PDF**. A PDF version of the results downloads to your machine.
## Visualization options
The results table has the following visualization options:
 * Probability density
 * Relative difference
 * Arm averages
###### Expand visualization options
### Probability density
The probability density graph displays the distribution of the results for each metric.
The horizontal x-axis displays the unit of the primary metric included in the experiment. For example, if the metric is measuring revenue, the unit might be dollars, or if the metric is measuring website latency, the unit might be milliseconds.
If the unit you’re measuring on the x-axis is something you want to increase, such as revenue, account sign ups, and so on, then the farther to the right the curve is, the better. The variation with the curve farthest to the right means the unit the metric is measuring is highest for that variation.
If the unit you’re measuring on the x-axis is something you want to decrease, such as website latency, then the farther to the left the curve is, the better. The variation with the curve farthest to the left means the unit the metric is measuring is lowest for that variation.
How wide a curve is on the x-axis determines the credible interval. Narrower curves mean the results of the variation fall within a smaller range of values, so you can be more confident in the likely results of that variation’s performance.
The vertical y-axis measures probability. You can determine how probable it is that the metric will equal the number on the x-axis by how high the curve is.
### Relative difference
The relative difference graph displays a time series of the relative difference between the treatment variation and the control. This graph is helpful for investigating trends in relative differences over time.
### Arm averages
The arm averages graph displays the average value over time for each variation. This graph is useful for investigating trends that impact all experiment variations equally over time.
## Filter options
You can filter an experiment’s results table by metric, variation, or attribute value.
![The filter options for an experiment.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/6c2d9f5c72467ac6c05a3708cbb6b75a65424f59751070ced9ab64fbfbffce18/assets/images/__toPlaywright_newIA/experiment-results-filters.png)
The filter options for an experiment.
###### Expand filter options
### Metrics filter
When you [create an experiment](/docs/home/experimentation/create), you can add one or more metrics for the experiment to measure. If an experiment is measuring more than one metric, you can filter the results to view only select metrics at a time. You cannot add additional metrics to an experiment after you create it.
To filter your results by metric, click **All metrics** and select the metric you want to view. The results table updates to show you results from only the metrics you selected.
### Variations filter
To filter your results by variation, click **All variations** and select the variations you want to view. The results table updates to show you results from only the variations you selected.
### Attributes filter
When you create an experiment, you can designate one or more [context attributes](/docs/home/experimentation/filters) as filterable for that experiment. If a context attribute is filterable, you can filter the experiment results by those attribute’s values. For example, if you designate the “Country” attribute as filterable, then you can narrow your results by users with a “Country” attribute value of `Canada`.
You cannot designate additional context attributes as filterable after you create an experiment.
To filter your results by context attribute value:
 * Click **All attributes**. A list of context attributes appears.
 * Hover on the context attribute you want to filter by. A list of available values for that attribute appears.
 * Search for and select the attribute values you want to view.
The results table updates to show you results from only contexts with the attribute value that you selected.
## Results chart data
This section explains the columns that display in the experiment results chart.
### Graph
The graph column displays the probability density, relative difference, or arm averages graph, depending on the visualization option you have selected. Click on the graph to view an enlarged version.
### Probability to be best
The probability to be best for a variation is the likelihood that it outperforms all other variations for a specific metric. However, the probability to be best alone doesn’t provide a complete view. For example, when multiple treatment variations outperform the control, the probability to be best may decrease, even if the treatments are generally effective.
To get a comprehensive understanding, pair the probability to be best with the [probability to beat control](/docs/home/experimentation/bayesian-results#probability-to-beat-control) and [expected loss](/docs/home/experimentation/bayesian-results#expected-loss). These metrics together provide a clearer picture of performance and trade-offs. To learn more, read [Decision making with Bayesian statistics](/docs/home/experimentation/bayesian).
### Probability to beat control
The probability to beat control represents the likelihood that this variation performs better than the control variation for a given metric. Probability to beat control is only relevant for treatment variations, as it measures the probability of outperforming the control, making it unnecessary for the control itself. When multiple treatment variations outperform the control, it’s helpful to also consider the [probability to be best](/docs/home/experimentation/bayesian-results#probability-to-be-best) to determine the winning variation.
#### Probability to beat control in funnel optimization experiments
###### Expand Probability to beat control in funnel optimization experiments
In funnel optimization experiments, the results table provides each variation’s probability to beat control for each step in the funnel, but the final metric in the funnel is the metric you should use to decide the winning variation for the experiment as a whole.
LaunchDarkly includes all end users that reach the last step in a funnel in the experiment’s winning variation calculations, even if an end user skipped some steps in the funnel. For example, if your funnel metric group has four steps, and an end user takes step 1, skips step 2, then takes steps 3 and 4, the experiment still considers the end user to have completed the funnel and includes them in the calculations for the winning variation.
### Relative difference
The relative difference from the control variation measures how much a metric in the treatment variation differs from the control variation, expressed as a proportion of the control’s estimated value. LaunchDarkly calculates this by taking the difference between the treatment variation’s estimated value and the control variation’s estimated value, then dividing that difference by the control variation’s estimated value.
Numbers in the relative difference column may display in green or red, depending on the variation’s probability to beat control:
 * **Green** : If the variation’s probability to beat control is higher than the threshold you set for the experiment, then the relative difference appears in green. It is highly likely that this treatment is superior to the control.
 * **Red** : If the variation’s probability to beat control is lower than 1 minus the threshold you set for the experiment, then the relative difference appears in red. It is highly likely that this treatment is inferior to the control.
### Expected loss
Ideally, shipping a winning variation would carry no risk. In reality, the probability for a treatment variation to beat the control variation is rarely 100%. This means there’s always some chance that a “winning” variation might not be an improvement over the control variation. To manage this, we need to measure the risk involved, which is called “expected loss.”
Expected loss represents the average potential downside of shipping a variation, quantifying how much one could expect to lose if it underperforms relative to the control variation. LaunchDarkly calculates this by integrating probability-weighted losses across all scenarios in which a given variation performs worse than control, with loss defined as the absolute difference between them.
A lower expected loss indicates lower risk, making it an important factor in choosing which variation to launch. The treatment variation with the highest probability to be best among those with a significant probability to beat control is generally considered the winner, but evaluating its expected loss clarifies the associated risk of implementing it.
For example, if you’re measuring conversion rate and have a winning variation with a 96% probability to beat control and an expected loss of 0.5%, this means there’s a strong likelihood of 96% that the winning variation will outperform the control variation. However, the 0.5% expected loss indicates that, on average, you’d expect a small 0.5% decrease in conversion rate if the winning variation were to underperform.
Expected loss does not display for [percentile metrics](/docs/home/metrics/metric-analysis#analysis-method).
### Exposures
The exposures column displays how many unique contexts encountered each variation of the experiment.
To learn more about troubleshooting if your experiment hasn’t received any metric events, read [Experimentation Results page status: “This metric has never received an event for this iteration”](https://support.launchdarkly.com/hc/en-us/articles/21910914554395-Experimentation-Results-page-status-This-metric-has-never-received-an-event-for-this-iteration).
## Metric-specific results
The remaining columns in a Bayesian experiment results chart vary depending on the metric in the experiment. Expand the sections below to learn about which columns display for each metric type.
### Binary conversion metrics
Binary conversion metrics include:
 * [Custom conversion binary](/docs/home/metrics/custom) metrics
 * [Clicked or tapped](/docs/home/metrics/click) metrics using the **Occurrence** option
 * [Page viewed](/docs/home/metrics/pageview) metrics using the **Occurrence** option
###### Expand Binary conversion metrics
#### Conversion rate (posterior mean)
The value for each unit in a binary conversion metric can be either 1 or 0. A value of 1 means the conversion occurred, such as a user viewing a web page, or submitting a form. A value of 0 means no conversion occurred.
The conversion rate column displays the posterior mean for the conversion metric, which is the percentage of units with at least one conversion that you should expect in this experiment, based on the data collected so far. For example, the percentage of users you can expect to click at least once.
##### The posterior conversion rate is not the raw conversion rate
The _raw_ conversion rate for an experiment is the number of conversions divided by the number of exposures. In Bayesian statistics, the _posterior_ conversion rate incorporates data the experiment has already collected to predict an expected conversion rate. For this reason, the posterior conversion rate may be different than the result of dividing the number of conversions by the number of exposures. If your experiment has 0 conversions so far, your posterior conversion rate may be higher than 0%, because it is the experiment’s expected conversion rate.
For funnel optimization experiments, the conversion rate includes all end users who completed the step, even if they didn’t complete a previous step in the funnel. LaunchDarkly calculates the conversion rate for each step in the funnel by dividing the number of end users who completed that step by the total number of end users who started the funnel. LaunchDarkly considers all end users in the experiment for whom the SDK has sent a flag evaluation event as having started the funnel.
#### Conversions
The conversions column displays the total number of users or other contexts that had at least one conversion.
### Count conversion metrics
Count conversion metrics include:
 * [Custom conversion count](/docs/home/metrics/custom-count) metrics
 * [Clicked or tapped](/docs/home/metrics/click) metrics using the **Count** option
 * [Page viewed](/docs/home/metrics/pageview) metrics using the **Count** option
###### Expand Count conversion metrics
#### Posterior mean
The value for each unit in a count conversion metric can be any positive value. The value equals the number of times the conversion occurred. For example, a value of 3 means the user clicked on a button three times.
The posterior mean is the average numeric value that you should expect in this experiment, based on the data collected so far. For example, the average number of times you can expect users to click on a button.
##### The posterior mean is not the same as the mean
The mean for a count conversion metric in an experiment is the average value of the metric. In Bayesian statistics, the _posterior_ mean incorporates data the experiment has already collected to predict an expected mean. For this reason, the posterior mean may be different than the mean value of the metric.
#### Total value
The total value is the sum total of all the numbers returned by the metric.
### Numeric metrics
###### Expand Numeric metrics
#### Posterior mean
The value for each unit in a numeric metric can be any positive value. The posterior mean is the variation’s average numeric value that you should expect in this experiment, based on the data collected so far.
##### The posterior mean is not the same as the mean
The mean for a numeric metric in an experiment is the average value of the metric. In Bayesian statistics, the _posterior_ mean incorporates data the experiment has already collected to predict an expected mean. For this reason, the posterior mean may be different than the mean value of the metric.
#### Total value
The total value is the sum total of all the numbers returned by the metric.
[![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)](/docs/home)
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs