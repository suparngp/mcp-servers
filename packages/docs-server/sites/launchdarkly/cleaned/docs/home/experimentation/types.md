`/`
[Product docs](/docs/home)[Guides](/docs/guides)[SDKs](/docs/sdk)[Integrations](/docs/integrations)[API docs](/docs/api)[Tutorials](/docs/tutorials)[Flagship Blog](/docs/blog)
 * Getting started
 * [Overview](/docs/home)
 * [Launch Insights](/docs/home/getting-started/launch-insights)
 * [LaunchDarkly architecture](/docs/home/getting-started/architecture)
 * [LaunchDarkly vocabulary](/docs/home/getting-started/vocabulary)
 * Feature flags and AI Configs
 * [Create flags](/docs/home/flags/create)
 * [Target with flags](/docs/home/flags/target)
 * [Flag templates](/docs/home/flags/templates)
 * [Manage flags](/docs/home/flags/manage)
 * [Code references](/docs/home/flags/code-references)
 * [AI Configs](/docs/home/ai-configs)
 * [Contexts](/docs/home/flags/contexts)
 * [Segments](/docs/home/flags/segments)
 * Releases
 * [Releasing features with LaunchDarkly](/docs/home/releases/releasing)
 * [Release policies](/docs/home/releases/release-policies)
 * [Percentage rollouts](/docs/home/releases/percentage-rollouts)
 * [Progressive rollouts](/docs/home/releases/progressive-rollouts)
 * [Guarded rollouts](/docs/home/releases/guarded-rollouts)
 * [Feature monitoring](/docs/home/releases/feature-monitoring)
 * [Release pipelines](/docs/home/releases/release-pipelines)
 * [Engineering insights](/docs/home/releases/eng-insights)
 * [Release management tools](/docs/home/releases/release-management)
 * [Applications and app versions](/docs/home/releases/apps-and-app-versions)
 * [Change history](/docs/home/releases/change-history)
 * Observability
 * [Observability](/docs/home/observability)
 * [Session replay](/docs/home/observability/session-replay)
 * [Error monitoring](/docs/home/observability/errors)
 * [Logs](/docs/home/observability/logs)
 * [Traces](/docs/home/observability/traces)
 * [LLM observability](/docs/home/observability/llm-observability)
 * [Alerts](/docs/home/observability/alerts)
 * [Dashboards](/docs/home/observability/dashboards)
 * [Search specification](/docs/home/observability/search)
 * [Observability settings](/docs/home/observability/settings)
 * [Vega](/docs/home/observability/vega)
 * Product analytics
 * [Product analytics](/docs/home/product-analytics)
 * [Setting up product analytics](/docs/home/product-analytics/setup)
 * [Using product analytics charts](/docs/home/product-analytics/chart)
 * Experimentation
 * [Experimentation](/docs/home/experimentation)
 * [Creating experiments](/docs/home/experimentation/create)
 * [Managing experiments](/docs/home/experimentation/manage)
 * [Analyzing experiments](/docs/home/experimentation/analyze)
 * [Experimentation and metric events](/docs/home/experimentation/events)
 * [Multi-armed bandits](/docs/home/multi-armed-bandits)
 * [Holdouts](/docs/home/holdouts)
 * Metrics
 * [Metrics](/docs/home/metrics)
 * [Metric groups](/docs/home/metrics/metric-groups)
 * [Autogenerated metrics](/docs/home/metrics/autogen-metrics)
 * [Metric impact](/docs/home/metrics/metric-impact)
 * [Metric events](/docs/home/metrics/metric-events)
 * Warehouse native
 * [Warehouse native Experimentation](/docs/home/warehouse-native)
 * [Warehouse native metrics](/docs/home/warehouse-native/metrics)
 * [Creating warehouse native experiments](/docs/home/warehouse-native/creating)
 * Infrastructure
 * [Connect apps and services to LaunchDarkly](/docs/home/infrastructure/apps)
 * [LaunchDarkly in China and Pakistan](/docs/home/infrastructure/china)
 * [LaunchDarkly in the European Union (EU)](/docs/home/infrastructure/eu)
 * [LaunchDarkly in federal environments](/docs/home/infrastructure/federal)
 * [Public IP list](/docs/home/infrastructure/ip-list)
 * Your account
 * [Projects](/docs/home/account/project)
 * [Views](/docs/home/account/views)
 * [Environments](/docs/home/account/environment)
 * [Tags](/docs/home/account/tags)
 * [Teams](/docs/home/account/teams)
 * [Members](/docs/home/account/members)
 * [Roles](/docs/home/account/roles)
 * [Account security](/docs/home/account/secure)
 * [Billing and usage](/docs/home/account/billing)
 * [Changelog](/docs/home/changelog)
[Sign in](/)[Sign up](https://app.launchdarkly.com/signup)
On this page
 * [Overview](#overview)
 * [Feature change experiments](#feature-change-experiments)
 * [Metrics](#metrics)
 * [Primary and secondary metrics](#primary-and-secondary-metrics)
 * [Funnel optimization experiments](#funnel-optimization-experiments)
 * [Metrics and metric groups](#metrics-and-metric-groups)
 * [Data Export experiments](#data-export-experiments)
 * [Warehouse native experiments](#warehouse-native-experiments)
 * [A/A tests](#aa-tests)
 * [AI Config experiments](#ai-config-experiments)
## Overview
This topic explains the different kinds of experiments you can run in LaunchDarkly, including feature change experiments, funnel optimization experiments, and Data Export experiments. Experiments let you measure the effect of flags or AI Configs on end users by mapping customer behavior to the metrics your team cares about. To learn more, read [Designing experiments](/docs/guides/experimentation/designing-experiments).
## Feature change experiments
Feature change experiments let you measure the effect different variations have on a metric. You can use feature change experiments to test a wide variety of feature changes. For examples of feature change experiments, read [Example experiments](/docs/guides/experimentation/example-experiments). Feature change experiments are also sometimes called “A/B tests.”
Here is an example **Results** tab for a feature change experiment:
![A feature change experiment's results tab.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/3bf9dbe90803123824c94c12296f8adf52ff7244905f747492bc2cf259dbac61/assets/images/__LD_UI_no_test/experiment-results-tab-full.png)
A feature change experiment's results tab.
You can run feature change experiments on both flags and AI Configs.
### Metrics
Metrics measure audience behaviors affected by the variations in your experiments. You can use metrics to track all kinds of things, from how often end users access a URL to how long that URL takes to load a page. If end users load a URL, click an element, or otherwise participate in the behavior the metric tracks, LaunchDarkly sends an event to your experiment.
You don’t need to create a new metric for each new experiment. You can reuse existing metrics in multiple experiments, which allows you to compare how the metric performs with different flags. Similarly, a single experiment can use primary and secondary metrics, which allow you to observe how the variations perform against various measurements.
LaunchDarkly can also automatically creates metrics based on events that your application sends to LaunchDarkly. To learn more, read [Autogenerated metrics](/docs/home/metrics/autogen-metrics).
### Primary and secondary metrics
You can designate only one metric as the primary metric in a feature change experiment, but you can attach secondary metrics to your experiments if you want to track the performance of additional measurements. We recommend using no more than ten metrics per experiment.
The primary metric is sometimes called the “overall evaluation criterion.” When you are making decisions about the winning variation in an experiment, you should base your decision-making only on your primary metric, because decision-making becomes much more complicated when you include multiple metrics in a decision.
When you create a feature change experiment, the primary metric is designated with a **crown** icon. After you add two or more metrics to an experiment, you can change the primary metric by clicking the **crown** icon of the new metric you want to designate as primary. To learn more, read [Creating feature change experiments](/docs/home/experimentation/feature).
![A new experiment with the primary metric designated by a "crown" icon.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/32665ffaa72f4b04d78eed92f499b2add96f1964f5cb5d0e534214a46fe65961/assets/images/__LD_UI_no_test/experiment-builder-select-metrics.png)
A new experiment with the primary metric designated by a "crown" icon.
If you are using just one metric, there are two possible outcomes: better or worse. If you are using two metrics, there are four possible outcome combinations: better/worse, worse/better, better/better, or worse/worse.
For each metric you add to an experiment, the possible outcomes increase quickly. If you are using three metrics, there are eight different possible outcome combinations. If you are using ten metrics, there are 1,024 possible outcome combinations. For this reason we recommend basing your decision-making on only your primary metric.
To learn how to create a new metric, read [Metrics](/docs/home/metrics).
## Funnel optimization experiments
A “funnel” is a marketing model that describes a customer’s journey through your purchasing or conversion cycle, typically from the awareness stage to the purchasing stage. LaunchDarkly’s funnel optimization experiments use multiple metrics to track the performance of each of the steps in your funnel over time.
Funnel optimization experiments let your product and marketing teams safely and rapidly increase the performance of your funnels by:
 * measuring the performance of your main acquisition and engagement channels
 * identifying strong points and weak points in your funnel flow
 * testing changes to ensure your funnel’s effectiveness improves over time
Before you build a funnel optimization experiment, you must first create a funnel metric group that includes metrics that measure all of the steps in your funnel.
You can run funnel optimization experiments on both flags and AI Configs.
### Metrics and metric groups
A metric group is a reusable, ordered list of metrics you can use to standardize metrics across multiple experiments. You can use funnel metric groups with funnel optimization experiments. To learn more, read [Metric groups](/docs/home/metrics/metric-groups).
Custom conversion binary metrics and clicked or tapped metrics are most often used with funnel optimization experiments. To learn more, read [Choose a metric type](/docs/home/metrics/create-metrics#choose-a-metric-type).
Funnel experiments can only use metrics that use the “Average” analysis method. To learn more, read [Analysis method](/docs/home/metrics/metric-analysis#analysis-method).
## Data Export experiments
Data Export experiments let you export experiment data to a third-party tool for analysis using [Data Export](/docs/integrations/data-export). Data Export experiments do not require a LaunchDarkly metric, and do not generate analysis results in the LaunchDarkly UI. This experiment type is useful when you want to use LaunchDarkly flags or AI Configs to manage your experiment, but have a preferred third-party analysis tool that allows you to import experiment data from other systems. To learn more, read [Creating Data Export experiments](/docs/home/experimentation/export).
You can run Data Export experiments on both flags and AI Configs.
## Warehouse native experiments
Warehouse native experiments let you connect LaunchDarkly to an external warehouse, such as Snowflake, and run experiments using events from that warehouse. To learn more, read [Warehouse native Experimentation](/docs/home/warehouse-native).
You can run funnel optimization experiments on flags, but not AI Configs.
## A/A tests
A/A tests are a type of feature change experiment that splits users into different, but identical, variations. When you run an A/A test, you compare two groups receiving the same product experience. This lets you validate that your experiment setup is working as intended, your metrics are tracking events as expected, and builds trust in your experiment results. If LaunchDarkly detects any unintended differences between variations, you can identify and correct sampling bias or configuration issues early.
You can run A/A tests on both flags and AI Configs.
## AI Config experiments
You can run each different kind of experiment that LaunchDarkly supports either on a flag or on an AI Config.
An AI Config is a resource that you create in LaunchDarkly and then use to customize, test, and roll out new large language models (LLMs) within your generative AI applications. For example, you might create an AI Config with the model configuration and messages that you use for a support bot in your application. You could create multiple variations using different models, model parameters, or messages, and run an experiment to determine which variation has the best end user satisfaction, or which variation uses the fewest output tokens.
As soon as you start using AI Configs in your application, LaunchDarkly automatically generates metrics for each variation. You can use these metrics in your experiments. You can also create your own additional metrics.
To learn more, read [AI Configs](/docs/home/ai-configs) and [Autogenerated metrics](/docs/home/metrics/autogen-metrics).
[![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)](/docs/home)
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs