`/`
[Product docs](/docs/home)[Guides](/docs/guides)[SDKs](/docs/sdk)[Integrations](/docs/integrations)[API docs](/docs/api)[Tutorials](/docs/tutorials)[Flagship Blog](/docs/blog)
 * Getting started
 * [Overview](/docs/home)
 * [Launch Insights](/docs/home/getting-started/launch-insights)
 * [LaunchDarkly architecture](/docs/home/getting-started/architecture)
 * [LaunchDarkly vocabulary](/docs/home/getting-started/vocabulary)
 * Feature flags and AI Configs
 * [Create flags](/docs/home/flags/create)
 * [Target with flags](/docs/home/flags/target)
 * [Flag templates](/docs/home/flags/templates)
 * [Manage flags](/docs/home/flags/manage)
 * [Code references](/docs/home/flags/code-references)
 * [AI Configs](/docs/home/ai-configs)
 * [Contexts](/docs/home/flags/contexts)
 * [Segments](/docs/home/flags/segments)
 * Releases
 * [Releasing features with LaunchDarkly](/docs/home/releases/releasing)
 * [Release policies](/docs/home/releases/release-policies)
 * [Percentage rollouts](/docs/home/releases/percentage-rollouts)
 * [Progressive rollouts](/docs/home/releases/progressive-rollouts)
 * [Guarded rollouts](/docs/home/releases/guarded-rollouts)
 * [Feature monitoring](/docs/home/releases/feature-monitoring)
 * [Release pipelines](/docs/home/releases/release-pipelines)
 * [Engineering insights](/docs/home/releases/eng-insights)
 * [Release management tools](/docs/home/releases/release-management)
 * [Applications and app versions](/docs/home/releases/apps-and-app-versions)
 * [Change history](/docs/home/releases/change-history)
 * Observability
 * [Observability](/docs/home/observability)
 * [Session replay](/docs/home/observability/session-replay)
 * [Error monitoring](/docs/home/observability/errors)
 * [Logs](/docs/home/observability/logs)
 * [Traces](/docs/home/observability/traces)
 * [LLM observability](/docs/home/observability/llm-observability)
 * [Alerts](/docs/home/observability/alerts)
 * [Dashboards](/docs/home/observability/dashboards)
 * [Search specification](/docs/home/observability/search)
 * [Observability settings](/docs/home/observability/settings)
 * [Vega](/docs/home/observability/vega)
 * Product analytics
 * [Product analytics](/docs/home/product-analytics)
 * [Setting up product analytics](/docs/home/product-analytics/setup)
 * [Using product analytics charts](/docs/home/product-analytics/chart)
 * Experimentation
 * [Experimentation](/docs/home/experimentation)
 * [Creating experiments](/docs/home/experimentation/create)
 * [Managing experiments](/docs/home/experimentation/manage)
 * [Analyzing experiments](/docs/home/experimentation/analyze)
 * [Experimentation and metric events](/docs/home/experimentation/events)
 * [Multi-armed bandits](/docs/home/multi-armed-bandits)
 * [Holdouts](/docs/home/holdouts)
 * Metrics
 * [Metrics](/docs/home/metrics)
 * [Metric groups](/docs/home/metrics/metric-groups)
 * [Autogenerated metrics](/docs/home/metrics/autogen-metrics)
 * [Metric impact](/docs/home/metrics/metric-impact)
 * [Metric events](/docs/home/metrics/metric-events)
 * Warehouse native
 * [Warehouse native Experimentation](/docs/home/warehouse-native)
 * [Warehouse native metrics](/docs/home/warehouse-native/metrics)
 * [Creating warehouse native experiments](/docs/home/warehouse-native/creating)
 * Infrastructure
 * [Connect apps and services to LaunchDarkly](/docs/home/infrastructure/apps)
 * [LaunchDarkly in China and Pakistan](/docs/home/infrastructure/china)
 * [LaunchDarkly in the European Union (EU)](/docs/home/infrastructure/eu)
 * [LaunchDarkly in federal environments](/docs/home/infrastructure/federal)
 * [Public IP list](/docs/home/infrastructure/ip-list)
 * Your account
 * [Projects](/docs/home/account/project)
 * [Views](/docs/home/account/views)
 * [Environments](/docs/home/account/environment)
 * [Tags](/docs/home/account/tags)
 * [Teams](/docs/home/account/teams)
 * [Members](/docs/home/account/members)
 * [Roles](/docs/home/account/roles)
 * [Account security](/docs/home/account/secure)
 * [Billing and usage](/docs/home/account/billing)
 * [Changelog](/docs/home/changelog)
[Sign in](/)[Sign up](https://app.launchdarkly.com/signup)
On this page
 * [Overview](#overview)
 * [Prerequisites](#prerequisites)
 * [Create flags or AI Configs](#create-flags-or-ai-configs)
 * [Limitations](#limitations)
 * [Create metrics](#create-metrics)
 * [Build A/A tests](#build-aa-tests)
 * [Turn on flags or AI Configs](#turn-on-flags-or-ai-configs)
 * [Start A/A test iterations](#start-aa-test-iterations)
## Overview
This topic explains how to set up and configure an A/A test in LaunchDarkly. A/A tests are a type of feature change experiment that splits users into different, but identical, variations. When you run an A/A test, you compare two groups receiving the same product experience. This lets you validate that your experiment setup is working as intended, your metrics are tracking events as expected, and builds trust in your experiment results.
Configuring an A/A test requires several steps:
 1. [Creating the flag or AI Config and its variations](/docs/home/experimentation/a-a-test#create-flags-or-ai-configs),
 2. [Creating one or more metrics](/docs/home/experimentation/a-a-test#create-metrics),
 3. [Building the A/A test](/docs/home/experimentation/a-a-test#build-aa-tests),
 4. [Turning on the flag or AI Config](/docs/home/experimentation/a-a-test#turn-on-flags-or-ai-configs), and
 5. [Starting an iteration](/docs/home/experimentation/a-a-test#start-aa-test-iterations).
These steps are explained in detail below.
### Prerequisites
Before you build an A/A test, you should read about and understand the following concepts:
 * [randomization units](/docs/home/experimentation/randomization)
 * [Bayesian and frequentist statistics](/docs/guides/experimentation/bayesian-frequentist)
## Create flags or AI Configs
To begin an A/A test, create a flag or AI Config with two variations. These variations must be defined identically to each other in your codebase.
##### A/A test variations must be identical
To run a successful A/A test, the two variations you use must be identical. Ensure that both variations are defined the same way in your code.
If you want to run an A/A test on a flag or AI Config with more than two variations, then at least two of the variations must be identical. Any additional variations can be unique. When you create the A/A test, you will not assign any traffic to those additional, unique variations.
You do not need to toggle on the flag before you create an A/A test, but you do have to toggle on the flag before you [start an experiment iteration](/docs/home/experimentation/a-a-test#start-aa-test-iterations). AI Configs are on by default.
To learn more, read [Creating new flags](/docs/home/flags/new), [Creating flag variations](/docs/home/flags/variations), [Create AI Configs](/docs/home/ai-configs/create), and [Create and manage AI Config variations](/docs/home/ai-configs/create-variation).
### Limitations
You cannot run an A/A test on a flag if:
 * the flag has an active [guarded rollout](/docs/home/releases/guarded-rollouts)
 * the flag has an active [progressive rollout](/docs/home/releases/progressive-rollouts)
 * the flag is in a running [Data Export experiment](/docs/home/experimentation/export)
 * the flag is in a running [warehouse native experiment](/docs/home/warehouse-native/creating)
 * the flag is a [migration flag](/docs/home/flags/migration)
You can build and run multiple funnel optimization experiments, feature change experiments, and A/A tests on the same flag or AI Config as long as there is only one running experiment per rule. You cannot run multiple experiments on the same rule at the same time.
## Create metrics
Metrics measure audience behaviors affected by your flags and AI Configs. You can use metrics to track all kinds of things, from how often end users access a URL to how long that URL takes to load a page. You can reuse metrics in multiple A/A tests, or create new ones for your A/A test.
To learn how to create your own new metric, read [Metrics](/docs/home/metrics). LaunchDarkly also automatically creates metrics for AI Configs. To learn more, read [Metrics generated from AI SDK events](/docs/home/metrics/autogen-metrics#metrics-autogenerated-from-ai-sdk-events).
##### Percentile analysis methods for Experimentation are in beta
The default metric analysis method is “Average.” The use of percentile analysis methods with LaunchDarkly experiments is in beta. If you use a metric with a percentile analysis method in a an experiment with a large audience, the experiment results tab may take longer to load, or the results tab may time out and display an error message. Percentile analysis methods are also not compatible with [CUPED adjustments](/docs/guides/statistical-methodology/cuped).
You can use one or more metrics or standard metric groups in A/A tests. However, you cannot use funnel metric groups in A/A tests. To learn more, read [Metric groups](/docs/home/metrics/metric-groups).
## Build A/A tests
You can view all of the experiments in your environment on the **Experiments** list.
To build an A/A test:
 1. Click **Create** and choose **Experiment**. The “Create experiment” dialog appears.
 2. Enter an A/A test **Name**.
 3. Enter a **Hypothesis**.
 4. Click **Create experiment**. The experiment **Design** tab appears.
 5. Select the **Feature change** experiment type.
 6. Check **Run as A/A test**.
 7. Choose a context kind to **Randomize by**.
 8. Select one or more **Metrics** or metric groups.
 * Hover over a metric to see which environments have received events for it. If no environments are receiving events, check that your SDKs are configured correctly.
 * Click **Create** to create and use a new metric or new [standard metric group](/docs/home/metrics/metric-groups).
 9. (Optional) If you have added multiple metrics and want to change the primary metric, hover on the metric name and click the **crown** icon.
![A metric group with the primary metric indicated.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/6cd23025e9765f768ec4b2df8ee0a3ae6bab3061e4c5b2f3a0eaa9bc3cad2eb1/assets/images/__toPlaywright_newIA/experiment-builder-primary-metric-callout.png)
A metric group with the primary metric indicated.
 1. Choose a **Flag or AI Config** to use in the A/A test. The flag or AI Config must have two identical variations.
 * Click **Create flag** or **Create AI Config** to create and use a new flag or AI Config.
 2. Choose a targeting rule for the **Experiment audience**.
 * If you want to restrict your A/A test audience to only contexts with certain attributes, create a targeting rule on the flag or AI Config you include in the A/A test and run the A/A test on that rule.
 * If you don’t want to restrict the audience for your A/A test, run the A/A test on the default rule. If the flag or AI Config doesn’t have any targeting rules, the default rule will be the only option.
![The "Audience targeting" section with the default rule chosen.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/39503b0adb7e876fa64cde29342d11ab9b06de5c05a2218fe8c06f7057729d77/assets/images/__toPlaywright_newIA/experiment-builder-define-audience.png)
The "Audience targeting" section with the default rule chosen.
 1. Choose the **Variation served to users outside this experiment**. Contexts that match the selected targeting rule but are not in the A/A test will receive this variation.
 2. Select the **Sample size** for the A/A test. This is the percentage of all of the contexts that match the A/A test’s targeting rule that you want to include in the A/A test.
 3. For A/A tests on flags or AI Configs with two variations, leave the variations split at 50%/50%.
 * If your flag or AI Config has more than two variations, click **Edit** to update the variation split. Assign 50% to each of the two identical variations, and 0% to any other variations.
 * Click **Save audience split**.
 4. Select a variation to serve as the **Control**.
 5. Select a **Statistical approach** of Bayesian or frequentist.
 * If you selected a statistical approach of Bayesian, select a preset or **Custom** success threshold.
 * If you selected a statistical approach of frequentist, select:
 * a **Significance level**.
 * a one-sided or two-sided **Direction of hypothesis test**.
###### Expand statistical approach options
You can select a statistical approach of **Bayesian** or **Frequentist**. Each approach includes one or more analysis options.
We recommend **Bayesian** when you have a small sample size of less than a thousand contexts, and we recommend **Frequentist** when you have a larger sample size of a thousand or more.
The Bayesian options include:
 * **Threshold** :
 * **90%** probability to beat control is the standard success threshold, but you can raise the threshold to **95%** or **99%** if you want to be more confident in your A/A test results.
 * You can lower the threshold to less than 90% using the **Custom** option. We recommend a lower threshold only when you are experimenting on non-critical parts of your app and are less concerned with determining a clear winning variation.
The frequentist options include:
 1. **Significance level** :
 * **0.05** p-value is the standard significance level, but you can lower the level to **0.01** or raise the level to **0.10** , depending on whether you need to be more or less confident in your results. A lower significance level means that you can be more confident in your winning variation.
 * You can raise the significance level to more than 0.10 using the **Custom** option. We recommend a higher significance level only when you are experimenting on non-critical parts of your app and are less concerned with determining a clear winning variation.
 2. **Direction of hypothesis test** :
 * **Two-sided** : We recommend two-sided when you’re unsure about whether the difference between the control and the treatment variations will be negative or positive, and want to look for indications of statistical significance in both directions.
 * **One-sided** : We recommend one-sided when you feel confident that the difference between the control and treatment variations will be either negative or positive, and want to look for indications of statistical significance only in one direction.
 3. (Optional) Select a [Multiple comparisons correction](/docs/guides/statistical-methodology/mcc) option:
 * Select **Apply across treatments** to correct for additional comparisons from multiple treatments
 * Select **Apply across metrics** to correct for additional comparisons across multiple metrics
 * Select **Apply across both metrics and treatments** to correct for additional comparisons from multiple metrics and multiple treatments
To learn more, read [Bayesian versus frequentist statistics](/docs/guides/experimentation/bayesian-frequentist).
 1. (Optional) If you want to be able to [filter your A/A test results by attribute](/docs/home/experimentation/filters), click **Advanced** , then select up to five context attributes to filter results by.
 2. (Optional) Click **Add tags** in the right sidebar to add new or existing tags to the A/A test.
 3. Scroll to the top of the page and click **Save**.
If needed, you can save your in-progress A/A test design to finish later. To save your design, click **Save** at the top of the creation screen. Your in-progress A/A test design is saved and appears on the **Experiments** list. To finish building the A/A test, click on the A/A test’s name and continue editing.
After you have created your A/A test, the next step is to toggle on the flag. AI Configs are on by default. Then, you can start an iteration.
![](https://fern-image-hosting.s3.us-east-1.amazonaws.com/launchdarkly/openapi-logo.svg)
You can also use the REST API: [Create experiment](/docs/api/experiments/create-experiment)
## Turn on flags or AI Configs
For an A/A test to begin recording data, the flag or AI Config used in the A/A test must be on. Targeting rules for AI Configs are on by default. To learn how to turn targeting rules on for flags, read [Turning flags on and off](/docs/home/flags/toggle).
## Start A/A test iterations
After you create an A/A test and toggle on the flag, you can start an A/A test iteration in one or more environments.
To start an A/A test iteration:
 1. Navigate to the **Experiments** list.
 2. Click on the environment section containing the A/A test you want to start.
 * If the environment you need isn’t visible, click the **+** next to the list of environment sections. Search for the environment you want, and select it from the list.
![The environment selection menu.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/34b4f0b3517652a1d52892a144f582c4acfeb5b49bfaf2ffee688fca284f3ce6/assets/images/auto/environment-selection.auto.png)
The environment selection menu.
 1. Click on the name of the A/A test you want to start an iteration for. The **Design** tab appears.
 2. Click **Start**.
 3. Repeat steps 1-4 for each environment you want to start an iteration in.
![An experiment with the "Start" button called out.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/4856586600e776039dac53473c7f3f577efab2d83cf765772a56660a4f528659/assets/images/__toPlaywright_newIA/experiment-details-start-callout.png)
An experiment with the "Start" button called out.
A/A test iterations allow you to record A/A tests in individual blocks of time. To ensure accurate A/A test results, when you make changes that impact an A/A test, LaunchDarkly starts a new iteration of the A/A test.
To learn more about starting and stopping iterations, read [Starting and stopping experiment iterations](/docs/home/experimentation/start-stop-exp).
![](https://fern-image-hosting.s3.us-east-1.amazonaws.com/launchdarkly/openapi-logo.svg)
You can also use the REST API: [Create iteration](/docs/api/experiments/create-iteration)
[![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)](/docs/home)
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs