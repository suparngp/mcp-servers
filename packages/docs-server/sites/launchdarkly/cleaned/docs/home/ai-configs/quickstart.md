`/`
[Product docs](/docs/home)[Guides](/docs/guides)[SDKs](/docs/sdk)[Integrations](/docs/integrations)[API docs](/docs/api)[Tutorials](/docs/tutorials)[Flagship Blog](/docs/blog)
 * Getting started
 * [Overview](/docs/home)
 * [Launch Insights](/docs/home/getting-started/launch-insights)
 * [LaunchDarkly architecture](/docs/home/getting-started/architecture)
 * [LaunchDarkly vocabulary](/docs/home/getting-started/vocabulary)
 * Feature flags and AI Configs
 * [Create flags](/docs/home/flags/create)
 * [Target with flags](/docs/home/flags/target)
 * [Flag templates](/docs/home/flags/templates)
 * [Manage flags](/docs/home/flags/manage)
 * [Code references](/docs/home/flags/code-references)
 * [AI Configs](/docs/home/ai-configs)
 * [Contexts](/docs/home/flags/contexts)
 * [Segments](/docs/home/flags/segments)
 * Releases
 * [Releasing features with LaunchDarkly](/docs/home/releases/releasing)
 * [Release policies](/docs/home/releases/release-policies)
 * [Percentage rollouts](/docs/home/releases/percentage-rollouts)
 * [Progressive rollouts](/docs/home/releases/progressive-rollouts)
 * [Guarded rollouts](/docs/home/releases/guarded-rollouts)
 * [Feature monitoring](/docs/home/releases/feature-monitoring)
 * [Release pipelines](/docs/home/releases/release-pipelines)
 * [Engineering insights](/docs/home/releases/eng-insights)
 * [Release management tools](/docs/home/releases/release-management)
 * [Applications and app versions](/docs/home/releases/apps-and-app-versions)
 * [Change history](/docs/home/releases/change-history)
 * Observability
 * [Observability](/docs/home/observability)
 * [Session replay](/docs/home/observability/session-replay)
 * [Error monitoring](/docs/home/observability/errors)
 * [Logs](/docs/home/observability/logs)
 * [Traces](/docs/home/observability/traces)
 * [LLM observability](/docs/home/observability/llm-observability)
 * [Alerts](/docs/home/observability/alerts)
 * [Dashboards](/docs/home/observability/dashboards)
 * [Search specification](/docs/home/observability/search)
 * [Observability settings](/docs/home/observability/settings)
 * [Vega](/docs/home/observability/vega)
 * Product analytics
 * [Product analytics](/docs/home/product-analytics)
 * [Setting up product analytics](/docs/home/product-analytics/setup)
 * [Using product analytics charts](/docs/home/product-analytics/chart)
 * Experimentation
 * [Experimentation](/docs/home/experimentation)
 * [Creating experiments](/docs/home/experimentation/create)
 * [Managing experiments](/docs/home/experimentation/manage)
 * [Analyzing experiments](/docs/home/experimentation/analyze)
 * [Experimentation and metric events](/docs/home/experimentation/events)
 * [Multi-armed bandits](/docs/home/multi-armed-bandits)
 * [Holdouts](/docs/home/holdouts)
 * Metrics
 * [Metrics](/docs/home/metrics)
 * [Metric groups](/docs/home/metrics/metric-groups)
 * [Autogenerated metrics](/docs/home/metrics/autogen-metrics)
 * [Metric impact](/docs/home/metrics/metric-impact)
 * [Metric events](/docs/home/metrics/metric-events)
 * Warehouse native
 * [Warehouse native Experimentation](/docs/home/warehouse-native)
 * [Warehouse native metrics](/docs/home/warehouse-native/metrics)
 * [Creating warehouse native experiments](/docs/home/warehouse-native/creating)
 * Infrastructure
 * [Connect apps and services to LaunchDarkly](/docs/home/infrastructure/apps)
 * [LaunchDarkly in China and Pakistan](/docs/home/infrastructure/china)
 * [LaunchDarkly in the European Union (EU)](/docs/home/infrastructure/eu)
 * [LaunchDarkly in federal environments](/docs/home/infrastructure/federal)
 * [Public IP list](/docs/home/infrastructure/ip-list)
 * Your account
 * [Projects](/docs/home/account/project)
 * [Views](/docs/home/account/views)
 * [Environments](/docs/home/account/environment)
 * [Tags](/docs/home/account/tags)
 * [Teams](/docs/home/account/teams)
 * [Members](/docs/home/account/members)
 * [Roles](/docs/home/account/roles)
 * [Account security](/docs/home/account/secure)
 * [Billing and usage](/docs/home/account/billing)
 * [Changelog](/docs/home/changelog)
[Sign in](/)[Sign up](https://app.launchdarkly.com/signup)
On this page
 * [Overview](#overview)
 * [Step 1, in your app: Install an AI SDK](#step-1-in-your-app-install-an-ai-sdk)
 * [Step 2, in LaunchDarkly: Create an AI Config](#step-2-in-launchdarkly-create-an-ai-config)
 * [Step 3, in LaunchDarkly: Set up targeting rules](#step-3-in-launchdarkly-set-up-targeting-rules)
 * [Step 4, in your app: Customize the AI Config, call your generative AI model, track metrics](#step-4-in-your-app-customize-the-ai-config-call-your-generative-ai-model-track-metrics)
 * [Customize the AI Config](#customize-the-ai-config)
 * [Call your generative AI model, track metrics](#call-your-generative-ai-model-track-metrics)
 * [Use Gemini prompt caching with AI Configs](#use-gemini-prompt-caching-with-ai-configs)
 * [Step 5, in LaunchDarkly: Monitor the AI Config](#step-5-in-launchdarkly-monitor-the-ai-config)
 * [Learn more about AI Configs](#learn-more-about-ai-configs)
 * [Integration with AI providers](#integration-with-ai-providers)
 * [Privacy and personally identifiable information (PII)](#privacy-and-personally-identifiable-information-pii)
 * [Availability of new models](#availability-of-new-models)
## Overview
This topic explains how to get started with the LaunchDarkly AI Configs product.
You can use AI Configs to customize, test, and roll out new large language models (LLMs) within your generative AI applications. AI Configs may be right for you if you want to:
 * Manage your model configuration and messages outside of your application code
 * Upgrade your app to the newest model version, then gradually release to your customers
 * Start using a new model provider, and progressively move your production traffic to the new provider
 * Compare the performance of different messages and models
AI Configs can be created in two modes:
 * Prompt-based: Configure prompts with messages and roles.
 * Agent-based: Configure multi-step workflows with instructions and tools. To learn more, read [Agents in AI Configs](/docs/home/ai-configs/agents).
##### This quickstart covers prompt-based configs
This quickstart focuses on prompt-based AI Configs. If you want to create and use agent-based configs, read [Agents in AI Configs](/docs/home/ai-configs/agents).
With AI Configs, both the messages and the model evaluation are specific to each end user, at runtime. You can update your messages, specific to each end user, without redeploying your application.
Working with AI Configs is available to members with a role that allows [AI Config actions](/docs/home/account/roles/role-actions#ai-config-actions). The LaunchDarkly Project Admin, Maintainer, and Developer project roles, as well as the Admin and Owner base roles, include this permission.
Follow the steps below to incorporate AI Configs into your app, or use the [in-app onboarding experience](https://app.launchdarkly.com/projects/default/onboarding) to set up your first AI Config directly in the LaunchDarkly UI.
If you’d prefer to learn from an example that’s built on a specific generative AI application, read one of our [guides](/docs/guides/ai-configs):
 * [Managing AI model configuration outside of code with the Node.js AI SDK](/docs/guides/ai-configs/config-outside-code-nodejs)
 * [Using targeting to manage AI model usage by tier with the Python AI SDK](/docs/guides/ai-configs/targeting-python)
You can use AI Configs with any model provider, including Gemini, OpenAI, and Anthropic. To learn how to use OpenAI or Anthropic models with AI Configs, read the examples in [Customize the AI Config](/docs/home/ai-configs/quickstart#customize-the-ai-config). To learn how to use Gemini models, including how to cache prompts, read [Use Gemini prompt caching with AI Configs](/docs/home/ai-configs/quickstart#use-gemini-prompt-caching-with-ai-configs).
##### Try the LaunchDarkly demo sandbox
You can also use the [LaunchDarkly demo sandbox](https://demo.app.launchdarkly.com/).
## Step 1, in your app: Install an AI SDK
First, install one of the [LaunchDarkly AI SDKs](/docs/sdk/ai) in your app:
.NET AI SDKGo AI SDKNode.js AI SDK (TypeScript)Python AI SDKRuby AI SDK
```
1
| Install-Package LaunchDarkly.ServerSdk
---|--- 
2
| Install-Package LaunchDarkly.ServerSdk.Ai
```
Next, import the LaunchDarkly AI client in your app and initialize a single, shared instance of it:
.NET AI SDKGo AI SDKNode.js AI SDK (TypeScript)Python AI SDKRuby AI SDK
```
1
| using LaunchDarkly.Sdk.Server.Ai;
---|--- 
2
| using LaunchDarkly.Sdk.Server.Ai.Adapters;
3
| using LaunchDarkly.Sdk.Server.Ai.Config;
4
| 
5
| var baseClient = new LdClient(Configuration.Builder("sdk-key-123").StartWaitTime(TimeSpan.FromSeconds(5)).Build());
6
| var aiClient = new LdAiClient(new LdClientAdapter(baseClient));
```
The AI SDKs each require that you specify your SDK key to authorize your application to connect to a particular environment within LaunchDarkly. SDK keys are specific to each project and environment. They are available from **Project settings** , on the **Environments** list in the LaunchDarkly UI. To learn more, read [Keys](/docs/sdk/concepts/client-side-server-side#keys).
Then, set up the context. Contexts are the people or resources who will encounter generated AI content in your application. The context attributes determine which variation of the AI Config LaunchDarkly serves to the end user, based on the targeting rules in your AI Config. If you are using template variables in the messages in your AI Config’s variations, the context attributes also fill in values for the template variables.
Here’s how:
.NET AI SDKGo AI SDKNode.js AI SDK (TypeScript)Python AI SDKRuby AI SDK
```
1
| using LaunchDarkly.Sdk;
---|--- 
2
| 
3
| var context = Context.Builder("example-user-key")
4
| .Kind("user")
5
| .Name("Sandy")
6
| .Build();
```
## Step 2, in LaunchDarkly: Create an AI Config
Next, create an AI Config in LaunchDarkly:
 1. Click **Create** and choose **AI Config**.
 2. In the “Create AI Config” dialog, select **Prompt-based** or **Agent-based**.
 * Prompt-based: Use messages and roles to configure prompts. Continue with this quickstart if you select this option.
 * Agent-based: Use instructions and tools to configure multi-step workflows. To learn more, read [Agents in AI Configs](/docs/home/ai-configs/agents).
![The "Create AI Config" dialog, with options for Prompt-based or Agent-based.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/c846083847b6de8473790bc958ea1bbb035e56f11187b0f07962f30c93223623/assets/images/__toPlaywright_newIA/ai-configs-create-agent.png)
The "Create AI Config" dialog, with options for Prompt-based or Agent-based.
 3. Enter a name for your AI Config, and optionally assign a maintainer.
 4. Click **Create**.
If you selected **Prompt-based** , the empty **Variations** tab of your new AI Config displays:
![The Variations tab of a newly-created AI Config.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/c5c632326b722032400458da26f5531c107fad51bee54dc70001d7f795b69a89/assets/images/auto/ai-config-variation-create.auto.png)
The Variations tab of a newly-created AI Config.
Then, create a variation. Every prompt-based AI Config has one or more variations. Each variation includes a model configuration and, optionally, one or more messages.
Here’s how:
 1. In the create panel in the **Variations** tab, replace “Untitled variation” with a variation **Name**. You’ll use this to refer to the variations when you set up targeting rules, below.
 2. Click **Select a model** and choose the model to use.
 * LaunchDarkly provides a list of common models, and updates it frequently.
 * You can also choose **+ Add a model** and create your own. To learn more, read [Create AI model configurations](/docs/home/ai-configs/create-model-config).
 3. (Optional) Select a **message role** and enter the message for the variation. If you’d like to customize the message at runtime, use `{{ example_variable }}` or `{{ ldctx.example_context_attribute }}` within the message. The LaunchDarkly AI SDK will substitute the correct values when you customize the AI Config from within your app.
 * To learn more about how variables and context attributes are inserted into messages at runtime, read [Customizing AI Configs](/docs/sdk/features/ai-config).
 4. Click **Review and save**.
You can also select **Import from playground** in your variation to bring your model, model parameters, and messages from an external playground.
Here’s an example of a completed variation:
![The Variations tab of an AI Config with one variation.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/b2c523c29b542ae6decbf7df90b6fd33b5b5915ca81f302664df8a56e052a880/assets/images/auto/ai-config-variation-complete.auto.png)
The Variations tab of an AI Config with one variation.
###### Expand to copy variation message
Here’s the variation message for this example. You can copy this if you’re working through this quickstart in your own project:
'Example jokes' variation, system message
```
 You are a system designed to tell knock-knock jokes. Each joke should be addressed to {{ ldctx.name }} and include at least one {{ example_variable }}. 
--- 
```
## Step 3, in LaunchDarkly: Set up targeting rules
Next, set up targeting rules for your AI Config. These rules determine which of your customers receive a particular variation of your AI Config.
To specify the AI Config variation to use by default when the AI Config is toggled on:
 1. Select the **Targeting** tab for your AI Config.
 2. In the “Default rule” section, click **Edit**.
 3. Configure the default rule to serve a specific variation.
 4. Click **Review and save**.
Here is an example of a default rule:
![An AI Config's default rule.](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a857891eb3c54434e42a02e0466e31724fb98c9542795d5526aef0361faea4d0/assets/images/auto/ai-config-default-rule.auto.png)
An AI Config's default rule.
The AI Config is enabled by default. After you’ve added code to your application to pull the customized messages and model configuration from LaunchDarkly, your AI Config will be active.
When an end user opens your application, they’ll get the AI Config variation you’ve defined, either in the default rule or in a custom targeting rule. Your app’s AI content generation will use the model and messages from the AI Config variation, and the messages will be customized for each end user.
When you’re ready to add more AI Config variations, come back to this step and set up additional targeting rules. If you are familiar with LaunchDarkly’s flag targeting, the process is very similar: with AI Configs, you can target individuals or segments, or target contexts with custom rules. To learn how, read [Target with AI Configs](/docs/home/ai-configs/target).
##### You can release variations gradually
You can release a new variation gradually using a [guarded rollout](/docs/home/releases/guarded-rollouts). Guarded rollouts reduce risk and let you monitor key metrics as the change rolls out.
## Step 4, in your app: Customize the AI Config, call your generative AI model, track metrics
Now that your AI Config is set up, you can use it in your app:
 1. [Customize the AI Config](/docs/home/ai-configs/quickstart#customize-the-ai-config): First, use the `config` function from the LaunchDarkly AI SDK to customize the AI Config. The `config` function returns the customized messages and model configuration along with a tracker instance for recording metrics.
 2. [Call your generative AI model, track metrics](/docs/home/ai-configs/quickstart#call-your-generative-ai-model-track-metrics): Then, call your generative AI model, passing in the result of the `config` function. In the LaunchDarkly AI SDKs, you use a `track[Model]Metrics` function to record metrics from your AI model generation. This function takes a completion from your AI model generation.
### Customize the AI Config
In your code, use the `config` function from the LaunchDarkly AI SDK to customize the AI Config. **You need to call`config` each time you generate content from your AI model**.
The `config` function returns the customized messages and model configuration along with a tracker instance for recording metrics. Customization means that any variables you include in the AI Config variation’s messages have their values set to the context attributes and variables you pass to the `config` function. Then, you can pass the customized messages directly to your AI. You should also set up a fallback value to use in case of an error, and make sure to handle the fallback case appropriately in your application.
Here’s how:
.NET AI SDKGo AI SDKNode.js AI SDKPython AI SDKRuby AI SDK
```
1
| var fallbackConfig = LdAiConfig.New()
---|--- 
2
| .SetEnabled(false)
3
| .Build();
4
| 
5
| var tracker = aiClient.Config(
6
| "ai-config-key-123abc",
7
| context,
8
| fallbackConfig,
9
| new Dictionary<string, object> {
10
| { "example_variable", "puppy" }
11
| }
12
| );
13
| 
14
| // Based on the example AI Config variation shown in step 2,
15
| // tracker.Config.Messages[0].Content will be:
16
| // "You are a model designed to tell knock-knock jokes. Each joke should be addressed to Sandy and include at least one puppy."
```
### Call your generative AI model, track metrics
Next, make a call to your generative AI model and pass in the result of the `config` function.
Use one of the `track[Model]Metrics` or `TrackRequest` functions to record metrics from your AI model generation. This function takes a completion from your AI model generation. Remember that you need to call `config` each time you generate content from your AI model.
LaunchDarkly provides specific functions for completions for several common AI model families, and an option to record this information yourself.
Here’s how to use a provider-specific `track[Model]Metrics` function to call a supported provider or framework, such as OpenAI, Amazon Bedrock, or LangChain, and record metrics from your AI model generation:
Node.js AI SDK (TypeScript), OpenAI modelNode.js AI SDK (TypeScript), Bedrock modelPython AI SDK, OpenAI modelPython AI SDK, Bedrock modelPython AI SDK, LangChain modelPython AI SDK, LangChain helper functionsPython AI SDK, Gemini modelRuby AI SDK, OpenAI modelRuby AI SDK, Bedrock modelRuby AI SDK, Bedrock helper function
```
1
| // Call config() each time you want to call the OpenAI operation
---|--- 
2
| 
3
| const aiConfig = aiClient.config(
4
| 'ai-config-key-123abc',
5
| context,
6
| fallbackConfig,
7
| { example_variable: 'elephant' },
8
| );
9
| 
10
| const { tracker } = aiConfig;
11
| 
12
| if (aiConfig.enabled) {
13
| try {
14
| const completion = await tracker.trackOpenAIMetrics(async () =>
15
| client.chat.completions.create({
16
| messages: aiConfig.messages || [],
17
| model: aiConfig.model?.name || 'gpt-4',
18
| temperature: (aiConfig.model?.parameters?.temperature as number) ?? 0.5,
19
| max_tokens: (aiConfig.model?.parameters?.maxTokens as number) ?? 4096,
20
| }),
21
| );
22
| 
23
| // use completion here
24
| } catch (err) {
25
| // handle exception from OpenAI or LaunchDarkly tracker
26
| }
27
| } else {
28
| // Application path to take when the aiConfig is disabled
29
| }
```
Here’s how to use the general `TrackRequest` function to call any AI model provider and record metrics from your AI model generation:
.NET AI SDK, any modelGo AI SDK, any model
```
1
| if (tracker.Config.Enabled == true) {
---|--- 
2
| 
3
| var response = tracker.TrackRequest(() =>
4
| {
5
| // Make request to a provider, which automatically tracks metrics in LaunchDarkly.
6
| // When sending the request to a provider, use details from tracker.Config.
7
| // For instance, you can pass tracker.Config.Model and tracker.Config.Messages.
8
| // Optionally, return response metadata, for example to do your own additional logging.
9
| //
10
| // CAUTION: If the call inside of Task.Run() throws an exception,
11
| // the SDK will re-throw that exception.
12
| 
13
| return new Response
14
| {
15
| Usage = new Usage { Total = 1, Input = 1, Output = 1 }, /* Token usage data */
16
| Metrics = new Metrics { LatencyMs = 100 } /* Metrics data */
17
| };
18
| }
19
| ));
20
| 
21
| } else {
22
| 
23
| // Application path to take when the tracker.Config is disabled
24
| 
25
| }
26
| 
27
| // Call Config() again each time you want to call the generative AI operation.
28
| var tracker = aiClient.Config(
29
| "ai-config-key-123abc",
30
| context,
31
| fallbackConfig,
32
| new Dictionary<string, object> {
33
| { "example_variable", "elephant" }
34
| }
35
| );
36
| 
37
| var response = tracker.TrackRequest(...)
```
Whether you use `track[Model]Metrics` or `TrackRequest`, the SDK automatically flushes these pending analytics events to LaunchDarkly at regular intervals. If you have a short-lived application, such as a script or unit test, you may need to explicitly request that the underlying LaunchDarkly client deliver any pending analytics events to LaunchDarkly, using [`flush()`](/docs/sdk/features/flush) or [`close()`](/docs/sdk/features/shutdown).
Here’s how:
.NET AI SDKGo AI SDKNode.js AI SDKPython AI SDKRuby AI SDK
```
1
| baseClient.Flush();
---|--- 
```
##### You can also track metrics yourself
LaunchDarkly provides specific functions for completions for several common AI model families, including OpenAI, Bedrock, and LangChain, and an option to record this information yourself.
If you are using another provider, or want to record additional metrics, each AI SDK also includes individual `track*` methods to record duration, token usage, generation success, generation error, time to first token, output satisfaction, and more. To learn more, read [AI metrics](/docs/sdk/features/ai-metrics).
##### LaunchDarkly AI SDK sample applications
For a complete example application, you can review some of our sample applications:
 * [Node.js AI SDK, using OpenAI](https://github.com/launchdarkly/js-core/tree/main/packages/sdk/server-ai/examples/openai)
 * [Node.js AI SDK, using Bedrock](https://github.com/launchdarkly/js-core/tree/main/packages/sdk/server-ai/examples/bedrock)
 * [Python AI SDK, using OpenAI](https://github.com/launchdarkly/hello-python-ai/blob/main/examples/openai_example.py)
 * [Python AI SDK, using Bedrock](https://github.com/launchdarkly/hello-python-ai/blob/main/examples/bedrock_example.py)
 * [Python AI SDK, using LangChain](https://github.com/launchdarkly/hello-python-ai/blob/main/examples/langchain_example.py)
 * [Python AI SDK, using Gemini](https://github.com/launchdarkly/hello-python-ai/blob/main/examples/gemini_example.py)
 * [Ruby AI SDK, using Bedrock](https://github.com/launchdarkly/ruby-server-sdk-ai/tree/main/examples/chatbot/aws-bedrock)
 * [Ruby AI SDK, using OpenAI](https://github.com/launchdarkly/ruby-server-sdk-ai/tree/main/examples/chatbot/openai)
### Use Gemini prompt caching with AI Configs
To use Gemini’s explicit prompt caching with AI Configs, we recommend structuring your variation with two system messages:
 1. Use the first system message (`messages[0]`) for static content that should be cached.
 2. Use the second system message (`messages[1]`) for dynamic content that changes per request, such as user input or context variables.
In your application, call `aiClient.config()` to evaluate the AI Config. Extract `messages[0]` and pass it to Gemini’s `system_instruction` field or use it when creating a cached prompt using the Gemini SDK. Use `messages[1]` as your dynamic prompt content.
To detect when the cached prompt should be refreshed, access `tracker.config.version`. This value changes when LaunchDarkly serves a different variation or when the variation’s content is updated. Alternatively, compare the `role` and content of `messages[0]` to a previously cached version.
## Step 5, in LaunchDarkly: Monitor the AI Config
Select the **Monitoring** tab for your AI Config. As end users use your application, LaunchDarkly monitors the performance of your AI Configs. Metrics are updated approximately every minute.
## Learn more about AI Configs
The following sections provide answers to common questions about working with AI Configs.
### Integration with AI providers
In the AI Configs product, LaunchDarkly does not handle the integration to the AI provider. The LaunchDarkly AI SDKs provide your application with model configuration details for providers and frameworks such as OpenAI, Amazon Bedrock, and LangChain, including customized messages and model parameters such as temperature and tokens. It is your application’s responsibility to pass this information to the AI provider or framework.
The LaunchDarkly AI SDKs provide methods to help you track how your AI model generation is performing, and in some cases, these methods take the completion from common AI providers as a parameter. However, it is still your application’s responsibility to call the AI provider. To learn more, read [Tracking AI metrics](/docs/sdk/features/ai-metrics).
### Privacy and personally identifiable information (PII)
LaunchDarkly does not send any of the information you provide to any models, and does not use any of the information to fine tune any models.
You should follow your own organization’s policies regarding if or when it may be acceptable to send end-user data either to LaunchDarkly or to an AI provider. To learn more, read [AI Configs and information privacy](/docs/home/ai-configs/privacy).
### Availability of new models
When you [create a new AI Config variation](/docs/home/ai-configs/create-variation), you can select a model from the provided list. LaunchDarkly updates this list regularly. To request a new model, click the **Give feedback** option and let us know what models you’d like to have included.
You can also add your own model at any time. To learn how, read [Create AI model configurations](/docs/home/ai-configs/create-model-config).
[![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)![Logo](https://files.buildwithfern.com/https://launchdarkly.docs.buildwithfern.com/docs/a8964c2c365fb94c416a0e31ff873d21ce0c3cbf40142e7e66cce5ae08a093af/assets/logo-dark.svg)](/docs/home)
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs
LaunchDarkly docs